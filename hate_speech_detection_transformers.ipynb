{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJEsUvyOMw1d"
   },
   "source": [
    "# Evaluating Cross-Lingual Transfer and Inoculation Strategies for Low-Resource Hate Speech Detection in Swedish\n",
    "\n",
    "Implement and evaluate:\n",
    "1. mBERT & XLM-R trained on English -> Evaluate on Swedish (Zero-shot Baseline).\n",
    "2. mBERT & XLM-R trained on English + All Other Non-Swedish Data -> Evaluate on Swedish.\n",
    "3. Inoculation: Fine-tune models from (1) and (2) with a small, fixed amount of Swedish data -> Evaluate on Swedish.\n",
    "4. KB-BERT trained on the same small amount of Swedish data -> Evaluate on Swedish.\n",
    "5. KB-BERT trained on all available Swedish training data -> Evaluate on Swedish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukpsw8Q0L2Nz"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Setting up my environment and installing relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQqso8N-wV0O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "DRIVE_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/'\n",
    "DATA_PATH = DRIVE_PATH + 'data/'\n",
    "OUTPUT_PATH = DRIVE_PATH + 'models/'\n",
    "RESULTS_PATH = DRIVE_PATH + 'results/'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMymtJfUM4eX"
   },
   "source": [
    "## Data Acquisition and Preparation\n",
    "\n",
    "Datasets:\n",
    "1. Swedish (Target): BiaSWE\n",
    "2. English (Base Training): EACL 2021 + Urban Dictionary Misogyny (Combined)\n",
    "3. Danish (Augmentation): DKhate\n",
    "4. German (Augmentation): GeRMS-AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939,
     "referenced_widgets": [
      "c7decc8516c04616832cd9ea09f12271",
      "2fd382b519214d81a672a843762e7a20",
      "755bcb8898ab486eacbfa0cd69b2ec53",
      "d192a7f68c044a54a4544f20cc76358d",
      "94469e7858974e37b501ebc05650183d",
      "5512305237b44bf4af2da677ce0f37f4",
      "37416dec7688466d932e9a26df9df641",
      "dfed3d6b37fe4eb6a47653b68d9ac18c",
      "a1131663b00d4d0681a167d4406a998c",
      "b97142c01f7245e6be5d61d28638a539",
      "e799409cb7a44f6a926980ffee80d565",
      "0b669514d44d4c8491a814c9180d2948",
      "f67ce3b668794cb5b56b1387ce67e386",
      "bda6cdf5818145f4838d63f4bf6619e8",
      "c8e8322c56944a3d97e63ee3706f9424",
      "4fa83f177de8494585cc52c4895aaaad",
      "e4698ff6823f44299f16994ee792860d",
      "13c8b19f1a9c4c2697afc96563522343",
      "74054dd671c042c4b27c58129abbb6f0",
      "895f10aef9454a7da19a9f1d778c0c0e",
      "35529c3f86ff4e6baef1911527ba2b10",
      "d84ff492834147238e839939daf5a236",
      "555df266e483481ca2e6176ba8f3b164",
      "260c1a518d7a41179ae6a8e4df15c34c",
      "ce77880fbdaa4d26a008282d41dccd8a",
      "b9387ac6c4554d33b7430d690e24cd51",
      "446a6994535d472b9c871eaaacebbf5d",
      "06c2c8392e1945bb93c89243f3529114",
      "01945f4c12ad46f48f7e172890b46ea4",
      "3a84a3485f9a4940951a25706a8f8036",
      "1cf7f7d74aef4b08ab02c04edc60a9a1",
      "1972de18b877411887db2d94d2908a7e",
      "310820658f124f4dbd7a89b8321129a5",
      "1c13c73a862c4df7bc741b6e3f8040cb",
      "b2d8513f5d3a40a7a10fe6d46583dc45",
      "0e6ae97667164d9e86988393dfd0c574",
      "2d0d6e379eca4ac997db89eecea5785f",
      "e62a32284a5d49979f65804319776a62",
      "057615140f024a8fa60c246adfb99615",
      "c3b5fdb3836747c6a99b50e182807eab",
      "7afcc5af988d4a4ab3fd73168624058b",
      "464eb43cd40949c1905fcb787a11b876",
      "517bf8c6e44d468185c5d5446189b06f",
      "ccc10a9f5490425aa678e9bef9d0263a",
      "576fb228fde44332b192f76ee33722b0",
      "2bacacf559314cd68f43799b0b70dd94",
      "df2539c400834fb496a4bc8e3072bb1b",
      "3c7aaceb13f74613b4bfecf757175f61",
      "3de8c00b9eea44bdafe0a0bca0fcf64a",
      "c2227ff5c93845c08cb5bcb9b0210a20",
      "e5c1851f3b8d4cd4962424a59310240c",
      "6ab9054b9d554e0085eef10f80b0ad02",
      "a8526f9f7d2c42128b8a15cc1fcff3df",
      "4cde873654f84aeca95799b774c599b7",
      "51bf2035a8f144efbf2ab76689cb85ef",
      "8601e7c1e1d749e8a852170c4243ab72",
      "4337986e3dfc466b957f43d972aaa704",
      "f5128bfc1a4c47fb87d973440e12cc83",
      "e1ba821da075462aaca16242baca90d6",
      "426cd5a53f4047c692eb241ba5d2f8c8",
      "e7f7c4b363774e678dee47c6885c82fa",
      "0db6a5e495f34e8db4edcb65613679c9",
      "dc6e4930df724c60847b1efa17c5cfaf",
      "93d0f81d99f0476ba62b44f9b82bcbbe",
      "c37c862a3a7a4ee386cc9bcec2efb07c",
      "4dc1c184d9bf4a2dab9b6df2618975f8",
      "007482e7cb914a92abbff9daf2ea8788",
      "11851a07af4f4f619e62dea4948a417f",
      "7f8c0a413ebb4ff88b2b7444477aa0d4",
      "0556f97369c64c32b5b044219f394992",
      "c895a948280847359c09af4eec159342",
      "be2b4bcffe9449d8983a57b6f3b13f6d",
      "f1a9721ab7774740a863e9cbbd6d84ed",
      "7c1adb084c164a1b8dbe6da9d6586cf9",
      "bf70695911b74528a1ca0799d6d97934",
      "af4f0a54d703450b824638b011ead1e2",
      "4ccb504025464625b729a386f91e0a74",
      "a5845bd49c3b477faa27560e791bd12c",
      "f1b2b659e7e440809da70e52a64196f3",
      "834c9c3c703b4d95a97125e3c5d21f6f",
      "1f58a2fe60464bb4b913eb643597c4c5",
      "ca7a8b2f3e87445286a24a9479fde36b",
      "82cd2d149020437f90dea21bdd3fbd53",
      "be0d7481072a401f9c036f71abb08d34",
      "d7c9e7e4f02f4b1ab5682280ddaf41e8",
      "5835af7af3c641e1a430b397632d3401",
      "b84179220389489c974182f0243d4791",
      "39296291c7c348d1a6f06e7ad0674e4f",
      "3799a5eabc284abf8baa7a11775f25bf",
      "9b683a1d7f71444cbcdcf331c9514223",
      "ebf805cea1964af6bbce20b7e7f1db9d",
      "2e1e43fa0d1a4df28114a9a04d441e62",
      "ce8143ed1166465393e6737275bdfbac",
      "b107c7c595c7459c9b4d2713dc425a63",
      "f70705d098234fcb8b335124edc60b07",
      "4cf67407065e41f7874fd402c7887f02",
      "f1dfeaa2c8e7488fbe13d82bd770d39a",
      "bbdd5b4ec3b24175a716d07b1e5f7b1c",
      "b4166ee40aa54b0195e6df8c72f69b0c",
      "4d489a449c224f2ba0e787686c0b08ac",
      "5eba64bf77f746ad92b0b427bdf403e7",
      "b372338cb0bd4c5e86be3fd297711a9d",
      "0570e09914bd4a789deb0a9d6dc86080",
      "345612d692b642cab130bf46adc37896",
      "ee34ad76c32445b0ad1242892b03a15b",
      "22e7fd86418a442fa558a13a855f205c",
      "0edfe88acf7746fe83d667c8690f3bf9",
      "cb58b74799fb4924afcca5f49d0b1f83",
      "20169d401bec48d88d4585421f2bdec4",
      "422d6116170347f6b5fa4f0eafb74990",
      "dfa9a641134d4c898cf0e59b0621a102",
      "87c67b7949c54ddbb94349fae4176c51",
      "bfbd533643c9422992226574cfa0ee42",
      "20d0e1e3e664495b8b51934b128ec3d0",
      "33afb3e988fa4ee2a02303bbac748727",
      "ac5cb6bd5ccc406396f32d56f350ef5d",
      "2cda5a1a653c4cb0bae6f7b94a132906",
      "4d3cd8ea849d41bf8714e83265e8bd91",
      "1103ff92f7654b89b1ec1021d03c3411",
      "b23f32ccea8c416bba36e52a81954324",
      "747a703f533b4a6388ff3f3ac0391e78",
      "367d69e6aa6643bc80cab5af9162aad8",
      "10e64168f14041c2bdb7f4dcf4d42e34",
      "01b73789c1cc43768de19c556a30d0c7",
      "119c38fce8784111803ab9b4f32779b8",
      "5f94e424faf94d08bb6979f409754900",
      "3a296b62400141f3b7a5b0e0e5ff9dcc",
      "1ced777d713c42f3b7144ebf3f925ff3",
      "bb3119cb680f4c5c81d98cfb46ba01d0",
      "e03f81989f72423baa39f977ff449f8d",
      "34ce6b11118f4b9888641c73f234a3f2",
      "66ed1e23f3724222992fd55d7d1a9706"
     ]
    },
    "id": "thMVW2seM9Il",
    "outputId": "1d7d5917-4195-47e9-81d9-66e6891a85a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Hub login potentially successful (or token provided).\n",
      "\n",
      "Loading datasets from Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded DKhate:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2960\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 329\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded BiaSWE:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'annotations'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'annotations'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'annotations'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authentication\n",
    "try:\n",
    "    login(token=\"INSERT TEXT\") # Insert personal token here\n",
    "    print(\"Hugging Face Hub login potentially successful (or token provided).\")\n",
    "except Exception as e:\n",
    "    print(f\"Hub login failed or token invalid: {e}\")\n",
    "\n",
    "print(\"\\nLoading datasets from Hugging Face Hub...\")\n",
    "\n",
    "# Load DKHate\n",
    "try:\n",
    "    dk_dataset_dict = load_dataset(\"DDSC/dkhate\")\n",
    "    print(\"\\nSuccessfully loaded DKhate:\")\n",
    "    print(dk_dataset_dict)\n",
    "    # Expected output: DatasetDict with 'train' and 'test' splits\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DKhate: {e}\")\n",
    "    dk_dataset_dict = None # Set to None if loading fails\n",
    "\n",
    "# Load BiaSWE\n",
    "try:\n",
    "    sw_dataset_dict = load_dataset(\"AI-Sweden-Models/BiaSWE\")\n",
    "    print(\"\\nSuccessfully loaded BiaSWE:\")\n",
    "    print(sw_dataset_dict)\n",
    "    # Expected output: DatasetDict possibly with 'train', 'validation', 'test' splits\n",
    "except Exception as e:\n",
    "    print(f\"Error loading BiaSWE: {e}\")\n",
    "    sw_dataset_dict = None # Set to None if loading fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr7qWFCu7p8j"
   },
   "source": [
    "## DKhate Dataset\n",
    "\n",
    "Upon inspection, the dataset seems imbalanced with roughly 87% class 0 vs. 13% class 1. In other words,  the neutral examples outnumber the hate-ridden speech examples.\n",
    "\n",
    "Things to consider/implement:\n",
    "* Class weights during training\n",
    "* Look at F-1 score (weighted or macro), precision, and recall when evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "6be66c4138f74d13b21fb3a3179bd0a0",
      "fc6350f6309b486a9f636d0628b8d2a1",
      "c87aaf9d15a54ab9bd940e2400e52d86",
      "16a914366c09412fb52a938d3daf672d",
      "404d6a7d8c41469bbcb787a9785759a6",
      "3266fc3f2d57407689fe69899c52b90b",
      "6dfd3cda4a92449c9047eb60517ded0f",
      "ce7529dbcae441fc9142fbf30125e5b5",
      "5492a0c9ebd24ecd92791c668e79ad77",
      "1a2cb70f8d2247ec9e57dd5489a5be0f",
      "89cf266d856d430eb9fd332551307b4b",
      "790b578cf3e84ab3a5a777ef73fc180e",
      "38528e4725af4bc28039f26694e2696b",
      "a276761ce9ce42c4a0a9d12635d76166",
      "8e7f090807bc4eaf9f57c893523bcd70",
      "93d4f3bfe6e9463db162a6738e324dba",
      "a9bf452dcd4e4db1869a9b5a200fee89",
      "169a3f8a44044fb1a329c0929e3ded4e",
      "2de79400efcf4f53afb6dfe133b836b5",
      "7de89c1ba6f8419395369925ab81dc68",
      "e62d6228fb364b52b14c12b1b0e393fa",
      "ec71341625b844759b266d702124360e"
     ]
    },
    "id": "n4HxhPIC4ren",
    "outputId": "d095e069-e13a-4a11-830e-6f8930589e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing DKhate DatasetDict...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DKhate preprocessing complete.\n",
      "Processed features (example from train split):\n",
      "{'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}\n",
      "\n",
      "Example processed data (first train example):\n",
      "{'text': 'jeg tror det vil være dejlig køligt, men jeg vil have det meget meget svært, såfremt personen i billedet skulle lægge softicen der. ', 'label': 0}\n",
      "\n",
      "Value counts for processed numerical label column (train split):\n",
      "{0: 2576, 1: 384}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "def preprocess_dkhate_dict(dataset_dict: DatasetDict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Preprocesses the DKhate DatasetDict loaded from Hugging Face Hub.\n",
    "\n",
    "    - Lowercases the 'text' column.\n",
    "    - Maps the original 'label' column (containing \"NOT\", \"OFF\")\n",
    "      to numerical values (0, 1) in the same 'label' column.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict: The raw DatasetDict object for DKhate (e.g., from load_dataset).\n",
    "\n",
    "    Returns:\n",
    "        A new DatasetDict with the processed data in all splits.\n",
    "        Returns None if input is not a DatasetDict.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset_dict, DatasetDict):\n",
    "        print(\"Error: Input must be a datasets.DatasetDict\")\n",
    "        return None\n",
    "\n",
    "    print(\"Preprocessing DKhate DatasetDict...\")\n",
    "\n",
    "    # Define the mapping for the labels\n",
    "    label_mapping = {\"NOT\": 0, \"OFF\": 1}\n",
    "\n",
    "    def map_labels_and_lowercase(example):\n",
    "        \"\"\"Internal function to process a single example dictionary.\"\"\"\n",
    "        # Lowercase text\n",
    "        # Check if 'text' exists and is a string before lowercasing\n",
    "        if 'text' in example and isinstance(example['text'], str):\n",
    "            example['text'] = example['text'].lower()\n",
    "        else:\n",
    "            # Handle cases where 'text' might be missing or not a string if necessary\n",
    "            example['text'] = \"\" # Assign empty string or handle as needed\n",
    "\n",
    "        # Map labels\n",
    "        original_label_key = 'label'\n",
    "        if original_label_key in example:\n",
    "            original_label_value = example[original_label_key]\n",
    "            # Overwrite the 'label' field with numerical mapping\n",
    "            # Use .get() with a default value (-1) for robustness against unexpected labels\n",
    "            example['label'] = label_mapping.get(original_label_value, -1)\n",
    "            # Warning for -1 values if they occur\n",
    "            if example['label'] == -1:\n",
    "                print(f\"Warning: Unexpected label value '{original_label_value}' found in DKhate example: {example}\")\n",
    "        else:\n",
    "            # Handle cases where the expected label column is missing\n",
    "            print(f\"Warning: Label key '{original_label_key}' not found in DKhate example: {example}\")\n",
    "            example['label'] = -1 # Assign default/error value\n",
    "\n",
    "        return example\n",
    "\n",
    "    # Use .map() method to apply the function to all examples in all splits\n",
    "    processed_dict = dataset_dict.map(\n",
    "        map_labels_and_lowercase,\n",
    "        batched=False, # Process example by example\n",
    "    )\n",
    "\n",
    "    print(\"DKhate preprocessing complete.\")\n",
    "\n",
    "    # Verify the feature type for the 'label' column changed\n",
    "    if 'train' in processed_dict: # Check a specific split if it exists\n",
    "         print(\"Processed features (example from train split):\")\n",
    "         print(processed_dict['train'].features)\n",
    "         # The 'label' feature should now show an integer type (e.g., int64)\n",
    "\n",
    "    return processed_dict\n",
    "\n",
    "processed_dk_dataset_dict = preprocess_dkhate_dict(dk_dataset_dict)\n",
    "if processed_dk_dataset_dict:\n",
    "  print(\"\\nExample processed data (first train example):\")\n",
    "  print(processed_dk_dataset_dict['train'][0])\n",
    "\n",
    "# Check label distribution in processed training data\n",
    "train_labels = processed_dk_dataset_dict['train']['label']\n",
    "label_counts = {}\n",
    "for label in train_labels:\n",
    "  label_counts[label] = label_counts.get(label, 0) + 1\n",
    "print(\"\\nValue counts for processed numerical label column (train split):\")\n",
    "print(label_counts) # Should show counts for 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpEGVtHTDx6P"
   },
   "source": [
    "## BiaSwe Dataset\n",
    "\n",
    "450 datapoints split into train, val, and test sets.\n",
    "\n",
    "The annotations contain hate-speech detection (\"yes\" or \"no\"), misogyny detection (\"yes\" or \"no\"), category detection (\"Stereotype\", \"Erasure and minimization\", \"Violence against women\", \"Sexualization and objectification\" and \"Anti-feminism and denial of discrimination\") and, finally, severity rating (on a scale of 1 to 10).\n",
    "\n",
    "Structure of each datapoint:\n",
    "{\"text\": \"...\", \"annotations\": {\"annotator 1\": {\"hate_speech\": \"...\", \"misogyny\": \"...\", \"category\": \"...\", \"rating\": \"...\", \"comment\": \"...\"}, \"annotatorevalleeedddccddddddkkkk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307,
     "referenced_widgets": [
      "f9965ef258004a3488facbe12c99cc61",
      "bc6f736d93424afd86e842520466a360",
      "7d42c136ec6549d3abb548ce09738f8e",
      "6cadc182ae8a4c92bbffc0d70091b5db",
      "ec1d20782b134964b35c6b4f83c9e1df",
      "f479fb5d509348228369b5906043672f",
      "e1257f7830384513b47aa7bea94aab76",
      "3e2966e8d1f24efdb8dee054a1bda0cd",
      "9744d64168934716976825021d7ad7a2",
      "099a52ee57b345ba9fb8af5d737f95c0",
      "15a4eec717654b81ae1311739eb2c634",
      "9f6c2e7fb5c148199c11d176d41a0cc3",
      "06c0b0036a764498adf5a1d4aae5499d",
      "efa85d2b771b48b6a24b95460fcc0206",
      "cbf6b32ca8804199af67bcffeec22545",
      "2bed428ddd0c4b49bba2335f6214a51e",
      "fc1563b6ef1a4016912b353959ad5bc3",
      "00b0f787d91f47d5b2f8890477a3bfdd",
      "320f1ca8e8a742a4a28e2414887c09ad",
      "c58c8392ee9941c2a607b27f93b777d3",
      "583b2d5b7207478395dcd9f1c7035f12",
      "83d0cb95e32545cdae28965e95c2031f",
      "5b7003295fb64b79adb173a4dda859ef",
      "7c1371f280b4469dbac6d38bac1ef573",
      "64911536abec48a8a7058d83b6c32827",
      "bdcd7e7d9bc44b2bb30afa8192c9a4c9",
      "35a511f7208d4ec19cab004d199ee361",
      "1d2a006b093544de8be5c7b1fe5a039d",
      "86369d1f55044315a2b2800e0e7ba833",
      "10cf2f929f8543ecb830a7a8dc9c5521",
      "ab1f0a7c954c491ea8a7d10f1cf9f3eb",
      "b18b02eeec2f4140872c9829e9a32c0a",
      "35ef2bf133ff499eba11ca87b92cca61"
     ]
    },
    "id": "A9xbs8vMD8bI",
    "outputId": "8a3b83d3-39eb-4c24-ca2e-47fd22e3a71c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing BiaSWE DatasetDict...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiaSWE preprocessing complete.\n",
      "Processed features (example from train split):\n",
      "{'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}\n",
      "\n",
      "Example processed data (first train example):\n",
      "{'text': 'ni som bor i hyreslägenhet! varför i helvete gör ni det? inte råd?: hej!  tycker de är allt för mycket folk som söker bostad och gnäller att det inte finns något.. köp en för helvete! vad gör ni av era pengar egentligen? så min fråga är varför köper inte fler personer lägenhet? varför super ni upp hela lönen istället för att spara till kontantinsats? eller trivs ni så bra i hyresghetton?', 'label': 0}\n",
      "\n",
      "Value counts for processed numerical label column (train split):\n",
      "{0: 80, 1: 70}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import DatasetDict, Dataset\n",
    "import json\n",
    "\n",
    "def preprocess_biaswe_dict(dataset_dict: DatasetDict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Preprocesses the BiaSWE DatasetDict.\n",
    "\n",
    "    - Extracts text (assuming top-level 'text' key).\n",
    "    - Lowercases text.\n",
    "    - Aggregates misogyny annotations using majority vote (conservative tie-breaking).\n",
    "    - Creates a final numerical 'label' column (0=Not Misogyny, 1=Misogyny).\n",
    "\n",
    "    Args:\n",
    "        dataset_dict: The raw DatasetDict object for BiaSWE.\n",
    "\n",
    "    Returns:\n",
    "        A new DatasetDict with processed data ('text', 'label').\n",
    "        Returns None if input is not a DatasetDict.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset_dict, DatasetDict):\n",
    "        print(\"Error: Input must be a datasets.DatasetDict\")\n",
    "        return None\n",
    "\n",
    "    print(\"Preprocessing BiaSWE DatasetDict...\")\n",
    "\n",
    "    annotator_keys = [\"annotator 1\", \"annotator 2\", \"annotator 3\", \"annotator 4\"]\n",
    "\n",
    "    def get_final_label_and_text(example):\n",
    "        \"\"\"Processes a single example to extract text and aggregate labels.\"\"\"\n",
    "        processed_example = {}\n",
    "\n",
    "        # Extract and lowercase text (assuming top-level 'text' key)\n",
    "        text_key = 'text' # Verify this key exists in dataset structure\n",
    "        if text_key in example and isinstance(example[text_key], str):\n",
    "            processed_example['text'] = example[text_key].lower()\n",
    "        else:\n",
    "            processed_example['text'] = \"\" # Handle missing/invalid text\n",
    "\n",
    "        # Aggregate Annotations\n",
    "        annotations_key = 'annotations' # Verify this key\n",
    "        yes_votes = 0\n",
    "        no_votes = 0\n",
    "        valid_votes = 0\n",
    "\n",
    "        if annotations_key in example and example[annotations_key]:\n",
    "            annotations = example[annotations_key]\n",
    "            # Handle if annotations are stored as a JSON string\n",
    "            if isinstance(annotations, str):\n",
    "                try:\n",
    "                    annotations = json.loads(annotations)\n",
    "                except json.JSONDecodeError:\n",
    "                    annotations = {} # Assign empty dict if JSON is invalid\n",
    "\n",
    "            if isinstance(annotations, dict): # Check if it's a dictionary\n",
    "                for key in annotator_keys:\n",
    "                    if key in annotations and annotations[key]: # Check if annotator exists and is not null\n",
    "                        annotator_data = annotations[key]\n",
    "                        # Check if 'misogyny' key exists within the annotator's data\n",
    "                        if isinstance(annotator_data, dict) and 'misogyny' in annotator_data:\n",
    "                           misogyny_label = annotator_data['misogyny']\n",
    "                           # Standardize potential labels (adjust based on actual values)\n",
    "                           if isinstance(misogyny_label, str):\n",
    "                               label_lower = misogyny_label.lower()\n",
    "                               if label_lower in ['yes', 'misogyny', '1']: # Add other positive variants if needed\n",
    "                                   yes_votes += 1\n",
    "                                   valid_votes += 1\n",
    "                               elif label_lower in ['no', 'not misogyny', '0']: # Add other negative variants\n",
    "                                   no_votes += 1\n",
    "                                   valid_votes += 1\n",
    "                               # Ignore NaN or empty strings implicitly\n",
    "\n",
    "        # Determine Final Label (Majority Vote, Conservative Tie-breaking)\n",
    "        if yes_votes > no_votes:\n",
    "            processed_example['label'] = 1\n",
    "        elif no_votes > yes_votes:\n",
    "            processed_example['label'] = 0\n",
    "        else: # Tie situation (includes 0 vs 0 if no valid votes)\n",
    "            processed_example['label'] = 0 # Conservative tie-breaking\n",
    "\n",
    "        return processed_example\n",
    "\n",
    "    # Use .map() to apply the function\n",
    "    # Create 'text' and 'label' from scratch based on complex logic.\n",
    "    processed_dict = dataset_dict.map(\n",
    "        get_final_label_and_text,\n",
    "        batched=False,\n",
    "        remove_columns=dataset_dict['train'].column_names # Remove all original columns\n",
    "    )\n",
    "\n",
    "    print(\"BiaSWE preprocessing complete.\")\n",
    "\n",
    "    # Verify features\n",
    "    if 'train' in processed_dict:\n",
    "         print(\"Processed features (example from train split):\")\n",
    "         print(processed_dict['train'].features)\n",
    "         # Should now only have 'text' (string) and 'label' (int64)\n",
    "\n",
    "    return processed_dict\n",
    "\n",
    "processed_sw_dataset_dict = preprocess_biaswe_dict(sw_dataset_dict)\n",
    "\n",
    "if processed_sw_dataset_dict:\n",
    "  print(\"\\nExample processed data (first train example):\")\n",
    "  print(processed_sw_dataset_dict['train'][0])\n",
    "\n",
    "# Check label distribution in processed training data\n",
    "train_labels = processed_sw_dataset_dict['train']['label']\n",
    "label_counts = {}\n",
    "for label in train_labels:\n",
    "  label_counts[label] = label_counts.get(label, 0) + 1\n",
    "print(\"\\nValue counts for processed numerical label column (train split):\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuV9hmUxFvzP",
    "outputId": "b29d7e46-ff37-4877-c997-0380279c436d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save processed datasets to: /content/drive/MyDrive/SwedishHateSpeechProject/processed_data/\n",
      "\n",
      "Variable 'processed_dk_dataset_dict' not found or not a DatasetDict. Skipping save.\n",
      "\n",
      "Variable 'processed_sw_dataset_dict' not found or not a DatasetDict. Skipping save.\n",
      "\n",
      "--- Saving Complete ---\n",
      "You should now find the folders 'dkhate_processed_hf' and 'biaswe_processed_hf'\n",
      "inside your '/content/drive/MyDrive/SwedishHateSpeechProject/processed_data/' directory on Google Drive.\n"
     ]
    }
   ],
   "source": [
    "# Saving Pre-processed Hugging Face Datasets\n",
    "\n",
    "DRIVE_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/'\n",
    "PROCESSED_DATA_PATH = DRIVE_PATH + 'processed_data/'\n",
    "\n",
    "# Create the base processed data directory if it doesn't exist\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "print(f\"Will save processed datasets to: {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "# Define specific paths for each dataset\n",
    "save_path_dk = os.path.join(PROCESSED_DATA_PATH, 'dkhate_processed_hf')\n",
    "save_path_sw = os.path.join(PROCESSED_DATA_PATH, 'biaswe_processed_hf')\n",
    "\n",
    "# Saving DKhate\n",
    "# Check if the processed dataset variable exists and is a DatasetDict\n",
    "if 'processed_dk_dataset_dict' in locals() and isinstance(processed_dk_dataset_dict, datasets.DatasetDict):\n",
    "    print(f\"\\nSaving processed DKhate dataset to: {save_path_dk}\")\n",
    "    try:\n",
    "        processed_dk_dataset_dict.save_to_disk(save_path_dk)\n",
    "        print(\"-> DKhate dataset saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error saving DKhate dataset: {e}\")\n",
    "else:\n",
    "    print(\"\\nVariable 'processed_dk_dataset_dict' not found or not a DatasetDict. Skipping save.\")\n",
    "\n",
    "# Saving BiaSWE\n",
    "# Check if the processed dataset variable exists and is a DatasetDict\n",
    "if 'processed_sw_dataset_dict' in locals() and isinstance(processed_sw_dataset_dict, datasets.DatasetDict):\n",
    "    print(f\"\\nSaving processed BiaSWE dataset to: {save_path_sw}\")\n",
    "    try:\n",
    "        processed_sw_dataset_dict.save_to_disk(save_path_sw)\n",
    "        print(\"-> BiaSWE dataset saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error saving BiaSWE dataset: {e}\")\n",
    "else:\n",
    "    print(\"\\nVariable 'processed_sw_dataset_dict' not found or not a DatasetDict. Skipping save.\")\n",
    "\n",
    "print(\"\\n--- Saving Complete ---\")\n",
    "print(\"You should now find the folders 'dkhate_processed_hf' and 'biaswe_processed_hf'\")\n",
    "print(f\"inside your '{PROCESSED_DATA_PATH}' directory on Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZq84wWYrjfs"
   },
   "source": [
    "## English Datasets\n",
    "\n",
    "This model training uses two English Datasets that will be concatenated to maximise the quantity of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxDSLnKJrilS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "DRIVE_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/'\n",
    "DATA_PATH = os.path.join(DRIVE_PATH, 'data/')\n",
    "PROCESSED_DATA_PATH = os.path.join(DRIVE_PATH, 'processed_data/')\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Define Input/Output Paths\n",
    "ud_csv_path = os.path.join(DATA_PATH, \"ManualTag_Misogyny.csv\") # Urban Dictionary\n",
    "eacl_csv_path = os.path.join(DATA_PATH, \"final_labels.csv\") # EACL\n",
    "\n",
    "save_path_ud = os.path.join(PROCESSED_DATA_PATH, 'ud_misogyny_processed_hf')\n",
    "save_path_eacl = os.path.join(PROCESSED_DATA_PATH, 'eacl_misogyny_processed_hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcPQF2gXFJN_"
   },
   "source": [
    "## Urban Dictionary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "WIbprCXvyD53",
    "outputId": "d3778f64-f50a-4990-8794-c46891580829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Urban Dictionary Misogyny from: /content/drive/MyDrive/SwedishHateSpeechProject/data/ManualTag_Misogyny.csv\n",
      "Loaded UD successfully. Shape: (2286, 2)\n",
      "Initial columns: Index(['Definition', 'is_misogyny'], dtype='object')\n",
      "Label value counts:\n",
      "is_misogyny\n",
      "0.0    1251\n",
      "1.0    1034\n",
      "Name: count, dtype: int64\n",
      "Data type of 'is_misogyny': float64\n",
      "Data type of 'Definition': object\n",
      "Warning: Found NaN values. Filling with 0.\n",
      "Updated label value counts: is_misogyny\n",
      "0    1252\n",
      "1    1034\n",
      "Name: count, dtype: int64\n",
      "Processed UD successfully. Shape: (2286, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2286 entries, 0 to 2285\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2286 non-null   object\n",
      " 1   label   2286 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 35.8+ KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"ud_processed_df\",\n  \"rows\": 2286,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2208,\n        \"samples\": [\n          \"the act of sexual intercourse in the urethra of a member of the female. pleasure is greatly increased when the penis enters the bladder. be warned that urethral sex, or u-sex as it is called on the seat, may result in unwanted urethral pregnancy and permenant physical damage.\",\n          \"the shittiest high school in plano independent school district. full of mexicans that think they are hot shit and teachers who dont know how to teach. it has the highest amount of kids who admit to smoking weed though. above 60 percent, at least we have that! the school mascot is an indian \\\"warrior\\\" and quite possibly is the most cliche thing in the world. williams is located in east plano, and is even ghetto by east plano standards. more than 20 different types of stds were found in the school with only one swab. embarassing.\",\n          \"the circumference of a female's upper arm can determine how much she will weigh in the future.  a larger circumference predicts a woman will gain weight uncontrollably as she ages, while a smaller circumference predicts that she will keep her petite figure without much effort.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "ud_processed_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5abd9add-2f34-4024-8d51-bec25d41690a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ur gonna die... queer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>valuptuous man boobs.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>variation of brother.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very impressive penis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what i call my penis.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5abd9add-2f34-4024-8d51-bec25d41690a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5abd9add-2f34-4024-8d51-bec25d41690a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5abd9add-2f34-4024-8d51-bec25d41690a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-834d965d-94f0-49c0-8ff0-acba8e66456e\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-834d965d-94f0-49c0-8ff0-acba8e66456e')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-834d965d-94f0-49c0-8ff0-acba8e66456e button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                    text  label\n",
       "0  ur gonna die... queer      0\n",
       "1  valuptuous man boobs.      0\n",
       "2  variation of brother.      0\n",
       "3  very impressive penis      0\n",
       "4  what i call my penis.      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_ud_misogyny(csv_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads and processes the simple UD misogyny dataset.\"\"\"\n",
    "    print(f\"\\nProcessing Urban Dictionary Misogyny from: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "        print(f\"Loaded UD successfully. Shape: {df.shape}\")\n",
    "        print(\"Initial columns:\", df.columns)\n",
    "\n",
    "\n",
    "        # Verify 'definition' and 'is_misogyny' columns exist and label is 0/1\n",
    "        if 'Definition' not in df.columns or 'is_misogyny' not in df.columns: # Adjust column names if needed\n",
    "             print(f\"!! Error: Expected 'Definition' and 'is_misogyny' columns not found in UD data. Please inspect.\")\n",
    "             return None\n",
    "        print(\"Label value counts:\")\n",
    "        print(df['is_misogyny'].value_counts()) # Check if it's already 0/1\n",
    "        print(f\"Data type of 'is_misogyny': {df['is_misogyny'].dtype}\")\n",
    "        print(f\"Data type of 'Definition': {df['Definition'].dtype}\")\n",
    "\n",
    "        # Check for missing values\n",
    "        if df['is_misogyny'].isnull().any():\n",
    "          print(\"Warning: Found NaN values. Filling with 0.\")\n",
    "          df.fillna({'is_misogyny':0}, inplace=True) # Replace the NaN with 0\n",
    "\n",
    "        # Ensure label is integer\n",
    "        df['is_misogyny'] = df['is_misogyny'].astype(int)\n",
    "        print(f\"Updated label value counts: {df['is_misogyny'].value_counts()}\")\n",
    "        # Clean Text (ensure string type first)\n",
    "        df['Definition'] = df['Definition'].astype(str).str.lower()\n",
    "\n",
    "        # Rename columns\n",
    "        df = df.rename(columns = {'Definition':'text', 'is_misogyny':'label'})\n",
    "        # Select final columns\n",
    "        final_df = df[['text', 'label']].copy()\n",
    "        print(f\"Processed UD successfully. Shape: {final_df.shape}\")\n",
    "        return final_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"!! Error: File not found at {csv_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error processing UD: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process and inspect\n",
    "ud_processed_df = process_ud_misogyny(ud_csv_path)\n",
    "ud_processed_df.info()\n",
    "ud_processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxuVOGkuLUwx"
   },
   "source": [
    "## Online Misogyny EACL 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vd_hClzxF7kX",
    "outputId": "8ce07616-7361-4c55-b89b-34c27f75de60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing EACL Misogyny from: /content/drive/MyDrive/SwedishHateSpeechProject/data/final_labels.csv\n",
      "Loaded EACL successfully. Shape: (6567, 18)\n",
      "\n",
      "Original EACL level_1 label value counts:\n",
      "level_1\n",
      "Nonmisogynistic    5868\n",
      "Misogynistic        699\n",
      "Name: count, dtype: int64\n",
      "Processed EACL successfully. Shape: (6567, 3)\n",
      "Final numerical label value counts:\n",
      "label\n",
      "0    5868\n",
      "1     699\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original split distribution:\n",
      "original_split\n",
      "train    5264\n",
      "test     1303\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def process_eacl_misogyny(csv_path: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads and processes the complex EACL misogyny dataset.\"\"\"\n",
    "    print(f\"\\nProcessing EACL Misogyny from: {csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded EACL successfully. Shape: {df.shape}\")\n",
    "        # Keep only potentially relevant columns\n",
    "        relevant_cols = ['body', 'level_1', 'split']\n",
    "        if not all(col in df.columns for col in relevant_cols):\n",
    "            print(f\"!! Error: Expected columns ('body', 'level_1', 'split') not found in EACL data.\")\n",
    "            print(\"Available columns:\", df.columns)\n",
    "            return None\n",
    "\n",
    "        df_subset = df[relevant_cols].copy()\n",
    "        print(\"\\nOriginal EACL level_1 label value counts:\")\n",
    "        print(df_subset['level_1'].value_counts())\n",
    "\n",
    "        # Rename columns\n",
    "        df_subset.rename(columns={'body': 'text', 'level_1': 'label_original', 'split': 'original_split'}, inplace=True)\n",
    "\n",
    "        # Map Labels\n",
    "        positive_label_str = 'Misogynistic'\n",
    "        label_map = {positive_label_str: 1, 'Nonmisogynistic': 0}\n",
    "\n",
    "        df_subset['label'] = df_subset['label_original'].map(label_map)\n",
    "\n",
    "        # Check for mapping errors (NaNs)\n",
    "        if df_subset['label'].isna().any():\n",
    "            print(\"!! Warning: Found NaN values in EACL 'label' column after mapping.\")\n",
    "            print(\"Original labels that failed to map:\")\n",
    "            print(df_subset[df_subset['label'].isna()]['label_original'].value_counts())\n",
    "            print(\"Dropping rows with mapping errors...\")\n",
    "            df_subset.dropna(subset=['label'], inplace=True)\n",
    "            df_subset['label'] = df_subset['label'].astype(int)\n",
    "\n",
    "        # Clean Text\n",
    "        df_subset['text'] = df_subset['text'].astype(str).str.lower()\n",
    "\n",
    "        # Select final columns (including the original split information)\n",
    "        final_df = df_subset[['text', 'label', 'original_split']].copy()\n",
    "        print(f\"Processed EACL successfully. Shape: {final_df.shape}\")\n",
    "        print(\"Final numerical label value counts:\")\n",
    "        print(final_df['label'].value_counts())\n",
    "        print(\"\\nOriginal split distribution:\")\n",
    "        print(final_df['original_split'].value_counts())\n",
    "        return final_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"!! Error: File not found at {csv_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error processing EACL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Call function to create Dataset\n",
    "eacl_processed_df = process_eacl_misogyny(eacl_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fTQfxPVLcEY"
   },
   "source": [
    "## Combining and Processing English Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990,
     "referenced_widgets": [
      "4a491565257042d890718c0bd0b954df",
      "2bf0bf9c21904f059c104f42ead2d4dc",
      "73f2195e70e944fdb07e593a0a6fa0db",
      "12faf6b7094c4f1a9b1170a291b77ec7",
      "0b5e04fd5aae4037b2f7129273ab3ecf",
      "a54129ff037c47ec93f86302c58ae1a0",
      "bf14cac5f37f4ead9be747e05265cd68",
      "94091850ae6d431782999fcef48f9da5",
      "15824edac68d45dba85b88b5683b3214",
      "1ab05cfc5b8d4f60bab398061d239a6e",
      "6057633a747c4ec49223c3a6ead6d693",
      "e6b329406b72411b979694ec26022f89",
      "a06f7c52e486460ba0867b3a9b7eda28",
      "49235dad8c9e40fea98c84249488fce2",
      "d5ae57ea0b9945c28d607444e26bdfce",
      "2f7a8494425644bab22cca38b09ed912",
      "70127b3ab54840829830aab83ad1351b",
      "2c51a49460904b96833d0a70bc8a5d39",
      "b2fa3e41eb3e4535b1cc5f5825ca3b02",
      "4a36a9a223f545bc8d256e3471de3a0b",
      "cf900fda0b1a4287a986de5b20b965d0",
      "d69a74618dba46f79f18166647b7ea84",
      "d5c86cf74f724aaf9c23ded8f94299d7",
      "7910ec1af12f469abd03470d2e3ad979",
      "4780957074ad4c6b917681182c334e42",
      "9545301a70de42f5b3d939ba331ae85f",
      "40b4e488600a41a6988f8d688e40e158",
      "86d20677031e43b29ca7857f989c66df",
      "ad3863836743440b852a900491776a8b",
      "2ad0969e51da4705a365ca954e415a96",
      "98cec6fa2c37416ca9bcb8becbc2818e",
      "c95c9947c65a4eeaa1287369645d415b",
      "0028a49b4b974923978f1398ab751b2d",
      "68ad105d57904507bbcf2adc37ca442e",
      "68e05d7f110d4425be27989b035937cc",
      "d42c31a64f7a4c10a5fca19ebc03ae26",
      "67a07d5dbb474a4a9933b6c22b607c48",
      "768bfd5ebabe433d912f1b5cf2d6af97",
      "75682a04c8da40568ba94b38d936f513",
      "a2592c4294ee4fa692f268e7b7cc8daa",
      "a7161d17e50d425c873d291835843565",
      "7e45d3d31c8640efaaa6deab7cbc347e",
      "e15dd14ee31e4baea695af3b2492ce74",
      "885d13e07923446289762fdd59c78cec",
      "11559b2805864b6ca4d1bbfb3bdee985",
      "774d307cf41f462d9b08a47cb82da8f7",
      "6ffae78dffc548fb9c096ebef497b790",
      "2f9dc1c18c3c40719f1eea7f645d54ee",
      "611c4ae4eecc4d8785ba2af14522f32a",
      "cdeb1173287e4094b2c5e9ef8286f26e",
      "30ccd26dc7624c2d9e4bdd09f6fc0d0d",
      "67d8853142994b8080cbdeb33e007508",
      "7df51205127b4359adfec8a675872fc7",
      "5f0b7e080fd14516beb260f569a18065",
      "386ef71bae1144b783f1270f89f075b0",
      "a4343da78dde479aace834872ec67f8c",
      "46bc0637ef6846ddb0a1ecda8e8782ad",
      "d0cc2bdbe03c4a5aa7847acf311344c9",
      "2ca81405e50f46f7b8b3c18a7df8eba4",
      "299e3d325fa542938f2fcf12144eca35",
      "ef68f9a5afc944638482b28867334be7",
      "f6ed2974139f40f1af859cacf4fa11d7",
      "09a658a96a284198be584e6d714b52dc",
      "bfca7f2dc1c6417fb0145166d89a539d",
      "f2667219f69743548be2d21d2c62669b",
      "7882b568c0d04732826ebd6e24e52947",
      "f31ad226b50849ab827baa9cb8399e55",
      "3c2356c92d3e435e8efa38cb846e10aa",
      "631e1cfe558140b59a68a19d6ef2b5e2",
      "4c02d0547d824d228a350e17b6223647",
      "768cc5ac780b4b469139a1dedea18ca2",
      "cbe55c669f5f41fa8aa58d79b4cb74d4",
      "57b7878f776e4ba6aee709473c036912",
      "b6e5429af73a457bb569f0d7cf4f7f32",
      "2c9e01c126334954903116378c7eed6c",
      "825b860143014e3a8faa6361d50d918f",
      "60225d491a0546b4aeb40e6555423212"
     ]
    },
    "id": "J211VGMpLa-x",
    "outputId": "e0e7e541-3906-41b7-f62b-7e397dbda4aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting English Dataset Processing ---\n",
      "\n",
      "Converting, splitting, and saving UD Misogyny...\n",
      "Creating new train/validation splits for UD Misogyny...\n",
      "Casting 'label' column to ClassLabel for stratification...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after casting: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['Not Problematic', 'Problematic'], id=None)}\n",
      "Splitting data (80% train / 20% validation) with stratification...\n",
      "\n",
      "Final splits for UD Misogyny: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1828\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 458\n",
      "    })\n",
      "})\n",
      "Saving processed UD Misogyny splits to: /content/drive/MyDrive/SwedishHateSpeechProject/processed_data/ud_misogyny_processed_hf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> UD Misogyny saved successfully.\n",
      "\n",
      "Converting, splitting, and saving EACL Misogyny...\n",
      "Using predefined splits for EACL Misogyny...\n",
      "Casting 'label' to ClassLabel for split 'train'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting 'label' to ClassLabel for split 'test'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after casting (example from first available split): {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['Not Problematic', 'Problematic'], id=None)}\n",
      "Warning: No 'validation' split found. Renaming 'test' split to 'validation'.\n",
      "\n",
      "Final splits for EACL Misogyny: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5264\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1303\n",
      "    })\n",
      "})\n",
      "Saving processed EACL Misogyny splits to: /content/drive/MyDrive/SwedishHateSpeechProject/processed_data/eacl_misogyny_processed_hf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> EACL Misogyny saved successfully.\n",
      "\n",
      "--- English Dataset Processing Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "def convert_split_save_english(df: pd.DataFrame | None, save_path: str, dataset_name: str, use_original_splits: bool = False):\n",
    "    \"\"\"\n",
    "    Converts DataFrame to Dataset, casts 'label' to ClassLabel,\n",
    "    handles splits (predefined or new), and saves the final DatasetDict.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(f\"\\nSkipping convert/split/save for {dataset_name}: Input DataFrame is None or empty.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nConverting, splitting, and saving {dataset_name}...\")\n",
    "    try:\n",
    "        # Define the ClassLabel feature once\n",
    "        # Use consistent names for easier concatenation later if needed elsewhere\n",
    "        class_label_feature = datasets.ClassLabel(num_classes=2, names=['Not Problematic', 'Problematic'])\n",
    "\n",
    "        # This variable will hold the final DatasetDict with train/validation splits\n",
    "        final_dataset_dict = None\n",
    "\n",
    "        # Using Predefined Splits (EACL)\n",
    "        if use_original_splits and 'original_split' in df.columns:\n",
    "            print(f\"Using predefined splits for {dataset_name}...\")\n",
    "            dataset_dict_temp = {} # Temporary dictionary to hold casted splits\n",
    "            for split_name in df['original_split'].unique():\n",
    "                 valid_split_name = split_name.lower()\n",
    "                 # Ensure standard split names\n",
    "                 if valid_split_name not in ['train', 'validation', 'test']:\n",
    "                     print(f\"Warning: Mapping original split '{split_name}' to 'train'.\")\n",
    "                     valid_split_name = 'train' # Default unknown splits to train\n",
    "\n",
    "                 # Create DataFrame for this specific split\n",
    "                 split_df = df[df['original_split'] == split_name].copy()\n",
    "                 split_df.drop(columns=['original_split'], inplace=True)\n",
    "\n",
    "                 if split_df.empty:\n",
    "                     print(f\"Warning: Split '{valid_split_name}' is empty. Skipping.\")\n",
    "                     continue\n",
    "\n",
    "                 # Convert DataFrame split to Dataset object\n",
    "                 split_ds = datasets.Dataset.from_pandas(split_df, preserve_index=False)\n",
    "\n",
    "                 # Cast the label column for this specific split Dataset\n",
    "                 print(f\"Casting 'label' to ClassLabel for split '{valid_split_name}'...\")\n",
    "                 try:\n",
    "                     split_ds = split_ds.cast_column(\"label\", class_label_feature)\n",
    "                     dataset_dict_temp[valid_split_name] = split_ds # Store the casted split\n",
    "                 except Exception as cast_error:\n",
    "                      print(f\"!! Error casting label for split '{valid_split_name}': {cast_error}\")\n",
    "                      # Skip split if invalid\n",
    "                      print(f\"!! Skipping split '{valid_split_name}' due to casting error.\")\n",
    "\n",
    "\n",
    "            if not dataset_dict_temp:\n",
    "                 print(f\"!! Error: No valid splits were processed for {dataset_name}. Cannot proceed.\")\n",
    "                 return\n",
    "\n",
    "            # Create the final DatasetDict from the successfully processed splits\n",
    "            final_dataset_dict = datasets.DatasetDict(dataset_dict_temp)\n",
    "            print(\"Features after casting (example from first available split):\", final_dataset_dict[list(final_dataset_dict.keys())[0]].features)\n",
    "\n",
    "            # Ensure 'validation' split exists (for predefined splits)\n",
    "            if 'train' not in final_dataset_dict:\n",
    "                 print(f\"!! Error: No 'train' split found after processing original splits for {dataset_name}. Cannot save.\")\n",
    "                 return # Cannot proceed without train split\n",
    "\n",
    "            if 'validation' not in final_dataset_dict:\n",
    "                 if 'test' in final_dataset_dict:\n",
    "                     print(\"Warning: No 'validation' split found. Renaming 'test' split to 'validation'.\")\n",
    "                     final_dataset_dict['validation'] = final_dataset_dict.pop('test')\n",
    "                 elif 'train' in final_dataset_dict:\n",
    "                     print(\"Warning: No 'validation' or 'test' split found. Creating 10% validation split from train.\")\n",
    "                     try:\n",
    "                         # Stratification requires ClassLabel\n",
    "                         train_val_split = final_dataset_dict['train'].train_test_split(test_size=0.1, stratify_by_column='label', seed=42)\n",
    "                         final_dataset_dict['train'] = train_val_split['train']\n",
    "                         final_dataset_dict['validation'] = train_val_split['test']\n",
    "                     except ValueError as e_stratify:\n",
    "                          print(f\"!! Error stratifying during validation split creation: {e_stratify}. Creating random split.\")\n",
    "                          train_val_split = final_dataset_dict['train'].train_test_split(test_size=0.1, seed=42)\n",
    "                          final_dataset_dict['train'] = train_val_split['train']\n",
    "                          final_dataset_dict['validation'] = train_val_split['test']\n",
    "                 else:\n",
    "                     # This case should be prevented by the 'train' check above\n",
    "                     print(\"!! Error: Cannot create validation split as no 'train' split exists.\")\n",
    "                     return\n",
    "\n",
    "\n",
    "        # Creating New Splits (UD)\n",
    "        else:\n",
    "            print(f\"Creating new train/validation splits for {dataset_name}...\")\n",
    "            # Convert entire DataFrame to Dataset\n",
    "            full_ds = datasets.Dataset.from_pandas(df, preserve_index=False) # Use a clear variable name 'full_ds'\n",
    "\n",
    "            # Cast Value label to ClassLabel before splitting\n",
    "            print(\"Casting 'label' column to ClassLabel for stratification...\")\n",
    "            try:\n",
    "                full_ds = full_ds.cast_column(\"label\", class_label_feature) # Cast the full dataset\n",
    "                print(f\"Features after casting: {full_ds.features}\")\n",
    "            except Exception as cast_error:\n",
    "                 print(f\"!! Error casting label for {dataset_name}: {cast_error}. Cannot proceed with stratification.\")\n",
    "                 return\n",
    "\n",
    "            # Split the casted dataset into train/validation\n",
    "            try:\n",
    "                print(\"Splitting data (80% train / 20% validation) with stratification...\")\n",
    "                split_dict = full_ds.train_test_split(test_size=0.2, stratify_by_column='label', seed=42)\n",
    "                split_dict['validation'] = split_dict.pop('test') # Rename 'test' from split to 'validation'\n",
    "                final_dataset_dict = datasets.DatasetDict(split_dict)\n",
    "            except ValueError as e_stratify:\n",
    "                 print(f\"!! Error stratifying during split: {e_stratify}. Creating random split.\")\n",
    "                 split_dict = full_ds.train_test_split(test_size=0.2, seed=42) # Fallback to random\n",
    "                 split_dict['validation'] = split_dict.pop('test')\n",
    "                 final_dataset_dict = datasets.DatasetDict(split_dict)\n",
    "\n",
    "        # Saving\n",
    "        if final_dataset_dict:\n",
    "            print(f\"\\nFinal splits for {dataset_name}: {final_dataset_dict}\")\n",
    "            print(f\"Saving processed {dataset_name} splits to: {save_path}\")\n",
    "            final_dataset_dict.save_to_disk(save_path)\n",
    "            print(f\"-> {dataset_name} saved successfully.\")\n",
    "        else:\n",
    "            print(f\"!! Skipping save for {dataset_name} as final DatasetDict was not created.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!! An unexpected error occurred during convert/split/save for {dataset_name}: {e}\")\n",
    "        traceback.print_exc() # Print detailed traceback for debugging\n",
    "\n",
    "\n",
    "print(\"--- Starting English Dataset Processing ---\")\n",
    "# Process UD (create new splits, will cast label inside function)\n",
    "convert_split_save_english(ud_processed_df, save_path_ud, \"UD Misogyny\", use_original_splits=False)\n",
    "# Process EACL (use predefined splits, will cast label inside function for each split)\n",
    "convert_split_save_english(eacl_processed_df, save_path_eacl, \"EACL Misogyny\", use_original_splits=True)\n",
    "print(\"\\n--- English Dataset Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk6uYCPiFBQa"
   },
   "source": [
    "## GeRMS-AT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D6hs6rdscd5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Configuration and Paths\n",
    "DRIVE_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/'\n",
    "DATA_PATH = os.path.join(DRIVE_PATH, 'data/')\n",
    "PROCESSED_DATA_PATH = os.path.join(DRIVE_PATH, 'processed_data/')\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "germs_path = os.path.join(DATA_PATH, \"germeval-competition-traindev.jsonl\")\n",
    "save_path_germs = os.path.join(PROCESSED_DATA_PATH, 'germs_at_processed_hf')\n",
    "\n",
    "\n",
    "def load_jsonl_robustly(file_path, encoding='utf-8'):\n",
    "    \"\"\"Loads a JSONL file line by line, skipping lines with decode errors.\"\"\"\n",
    "    data_list = []\n",
    "    print(f\"Attempting robust load for: {file_path}\")\n",
    "    line_num = 0\n",
    "    skipped_lines = 0\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            for line in f:\n",
    "                line_num += 1\n",
    "                if not line.strip(): continue # Skip empty lines\n",
    "                try:\n",
    "                    data_list.append(json.loads(line))\n",
    "                except json.JSONDecodeError as jde:\n",
    "                    print(f\"!! JSON Decode Error on line {line_num}: {jde}. Skipping line.\")\n",
    "                    skipped_lines += 1\n",
    "                    continue\n",
    "        print(f\"Successfully parsed {len(data_list)} lines. Skipped {skipped_lines} lines.\")\n",
    "        if not data_list: return None\n",
    "        return pd.DataFrame(data_list)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"!! Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error during manual line-by-line reading: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963,
     "referenced_widgets": [
      "da17560bec564aae82f7844af356f3b2",
      "886b79e280ef486c94cbbc637197cb1f",
      "a3132d87c21145b7b82eb5f2b4d3580d",
      "0b1cae5164594ee783b2a24ba050c39d",
      "9a26bc5706364d559e7edc9283997bc8",
      "cc1a72be841043aa8b7b2ec8d86e9208",
      "da60e135b45945f7a420858c6e7307b4",
      "e227703aed5d4b81886529e545542d5a",
      "dafb29201d594233853c8f946b16cb81",
      "f64335f99cd042b78ed5263410d660de",
      "d0f234241a24462786483457481fe6ae",
      "a2dbabf2ec9e49e49c6eb0ddbbf73105",
      "0240091a261a4f9abdb5ee515aa4ec1d",
      "6416f119f04a4efc9837fa4aa68ce6c5",
      "a051f7afcb2d45969abc2fd1cf37f958",
      "16814a33fb64468b94112bcaae64e9d7",
      "c234e8eac1aa4c9b9264e86d89f27370",
      "c2cd1788715e456eb10e0bba02030adc",
      "a51e7bde53a74109b8f4104ca83b1f52",
      "387954903af74e579023a4b6a18f7ddb",
      "f581e1d663a044ae95628a2d6b14b233",
      "cf71c855b659421585d2190aa500b52a",
      "d174df881edf4386a1cb4f6a5bb2eb3c",
      "a0e74c61dc844933a63944253381c14a",
      "e425b139fccc44228a26fae65eb9c687",
      "75779fda6d25415ba1019c0b7590dcca",
      "0547ebe0839845c687295f57f78d68ed",
      "76e87afd7a1644a3b2baded0bfe82026",
      "122180d43a74410492a2759979ed2392",
      "749fa02b01aa459ba4f21bfe92b317a4",
      "ba259853c2ac413a90bf6a4fb693efb4",
      "af0fde8041e84413acd12e75fef98861",
      "7b8b63bed0d949cf80aa20a8e57b66c9"
     ]
    },
    "id": "lHu5efxrF9ss",
    "outputId": "9e70ada9-9cc5-41f3-b23e-eff65e1c3b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading GeRMS-AT Data ---\n",
      "Attempting robust load for: /content/drive/MyDrive/SwedishHateSpeechProject/data/germeval-competition-traindev.jsonl\n",
      "Successfully parsed 5998 lines. Skipped 0 lines.\n",
      "\n",
      "--- Applying Processing Functions ---\n",
      "\n",
      "Processing GeRMS-AT Train DataFrame (using text='text', annotations='annotations')...\n",
      "\n",
      "Processed GeRMS-AT Train successfully. Final Shape: (5998, 2)\n",
      "Final numerical label value counts:\n",
      "label\n",
      "0    5675\n",
      "1     323\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Creating Final DatasetDict ---\n",
      "\n",
      "--- Applying Processing Function to GeRMS-AT Train Data ---\n",
      "\n",
      "Processing GeRMS-AT Train DataFrame (using text='text', annotations='annotations')...\n",
      "\n",
      "Processed GeRMS-AT Train successfully. Final Shape: (5998, 2)\n",
      "Final numerical label value counts:\n",
      "label\n",
      "0    5675\n",
      "1     323\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Splitting Processed GeRMS-AT Train into Train/Validation ---\n",
      "Casting label to ClassLabel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after casting: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['Not Problematic', 'Problematic'], id=None)}\n",
      "Splitting into train (80%) / validation (20%)...\n",
      "Successfully created GeRMS-AT DatasetDict with Train/Validation splits:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4798\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Saving Processed GeRMS-AT Train/Validation Splits ---\n",
      "Saving processed GeRMS-AT dataset to: /content/drive/MyDrive/SwedishHateSpeechProject/processed_data/germs_at_processed_hf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> GeRMS-AT dataset saved successfully.\n",
      "\n",
      "--- GeRMS-AT Dataset Processing Complete (Train/Validation Splits Created) ---\n"
     ]
    }
   ],
   "source": [
    "# Processing Function for GeRMS-AT\n",
    "def process_germs_jsonl_df(df: pd.DataFrame, df_name: str,\n",
    "                           text_col: str = 'text', # Default text col name\n",
    "                           annotation_col: str = 'annotations' # Default annotation col name\n",
    "                           ) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Processes a GeRMS-AT DataFrame (train or test) from JSONL.\n",
    "    Handles different annotation column names.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(f\"Skipping processing for {df_name} as input DataFrame is None or empty.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nProcessing {df_name} DataFrame (using text='{text_col}', annotations='{annotation_col}')...\")\n",
    "    try:\n",
    "        # Verify columns exist using the passed arguments\n",
    "        if text_col not in df.columns or annotation_col not in df.columns:\n",
    "            print(f\"!! Error: Expected columns '{text_col}' and '{annotation_col}' not found in {df_name}.\")\n",
    "            print(\"Available columns:\", df.columns)\n",
    "            return None\n",
    "\n",
    "        # Label Mapping\n",
    "        vote_mapping = {\n",
    "            '0-Kein': 0, '1-Implizit': 1, '2-Explizit': 1, '3-Verdeckt': 1, '4-Extrem': 1\n",
    "        }\n",
    "\n",
    "        processed_rows = []\n",
    "        for index, row in df.iterrows():\n",
    "            processed_row = {}\n",
    "            processed_row['text'] = str(row[text_col]).lower() if pd.notna(row[text_col]) else \"\"\n",
    "\n",
    "            # Aggregate Annotations - Using the passed annotation_col name\n",
    "            annotations_data = row[annotation_col]\n",
    "            yes_votes, no_votes, valid_votes = 0, 0, 0\n",
    "\n",
    "            # Check if test data has label info or just annotator list\n",
    "            # If the first item in annotations_data is dict -> likely has labels\n",
    "            # If the first item is string -> likely just annotator IDs\n",
    "            has_labels_in_test = False\n",
    "            if isinstance(annotations_data, list) and annotations_data:\n",
    "                 if isinstance(annotations_data[0], dict):\n",
    "                      has_labels_in_test = True\n",
    "                 # Add specific check for the 'annotators' column if needed\n",
    "                 elif annotation_col == 'annotators' and isinstance(annotations_data[0], str):\n",
    "                      print(f\"Info: '{annotation_col}' column in {df_name} appears to contain only annotator IDs, cannot derive label.\")\n",
    "                      has_labels_in_test = False # Explicitly set\n",
    "\n",
    "            if has_labels_in_test: # Only process if label structures\n",
    "                 # Handle potential JSON string representation\n",
    "                 if isinstance(annotations_data, str):\n",
    "                     try: annotations_data = json.loads(annotations_data)\n",
    "                     except json.JSONDecodeError: annotations_data = []\n",
    "\n",
    "                 if isinstance(annotations_data, list):\n",
    "                     for annotation in annotations_data:\n",
    "                         if isinstance(annotation, dict) and 'label' in annotation:\n",
    "                             original_label = annotation['label']\n",
    "                             vote = vote_mapping.get(original_label)\n",
    "                             if vote == 1: yes_votes += 1; valid_votes += 1\n",
    "                             elif vote == 0: no_votes += 1; valid_votes += 1\n",
    "\n",
    "                 # Determine Final Label based on votes\n",
    "                 if yes_votes > no_votes: processed_row['label'] = 1\n",
    "                 else: processed_row['label'] = 0 # Default to 0 if tie or no labels found\n",
    "            else:\n",
    "                 # Assigning -1 is common for unlabeled test sets.\n",
    "                 processed_row['label'] = -1 # Indicate missing label for test set\n",
    "\n",
    "            processed_rows.append(processed_row)\n",
    "\n",
    "        final_df = pd.DataFrame(processed_rows)\n",
    "\n",
    "        if final_df.empty:\n",
    "             print(f\"Warning: Resulting DataFrame for {df_name} is empty.\")\n",
    "             return None\n",
    "\n",
    "        print(f\"\\nProcessed {df_name} successfully. Final Shape: {final_df.shape}\")\n",
    "        print(\"Final numerical label value counts:\")\n",
    "        print(final_df['label'].value_counts()) # Check if -1 appears for test set\n",
    "        return final_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error during processing {df_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load Data\n",
    "print(\"--- Loading GeRMS-AT Data ---\")\n",
    "df_train_germs = load_jsonl_robustly(germs_path)\n",
    "\n",
    "# Apply Processing with Correct Column Names\n",
    "print(\"\\n--- Applying Processing Functions ---\")\n",
    "# For Train data, specify annotation_col='annotations'\n",
    "processed_train_germs_df = process_germs_jsonl_df(df_train_germs, \"GeRMS-AT Train\",\n",
    "                                                   text_col='text',\n",
    "                                                   annotation_col='annotations')\n",
    "\n",
    "# Combine into DatasetDict\n",
    "print(\"\\n--- Creating Final DatasetDict ---\")\n",
    "final_germs_dataset_dict = None\n",
    "\n",
    "print(\"\\n--- Applying Processing Function to GeRMS-AT Train Data ---\")\n",
    "processed_train_germs_df = process_germs_jsonl_df(df_train_germs, \"GeRMS-AT Train\",\n",
    "                                                   text_col='text',\n",
    "                                                   annotation_col='annotations')\n",
    "\n",
    "\n",
    "# Split Processed Train Data into Train/Validation\n",
    "print(\"\\n--- Splitting Processed GeRMS-AT Train into Train/Validation ---\")\n",
    "final_germs_dataset_dict = None\n",
    "if processed_train_germs_df is not None and not processed_train_germs_df.empty:\n",
    "    try:\n",
    "        # Convert full processed train DataFrame to Dataset\n",
    "        full_train_ds = datasets.Dataset.from_pandas(processed_train_germs_df, preserve_index=False)\n",
    "\n",
    "        # Cast label to ClassLabel FOR STRATIFICATION\n",
    "        print(\"Casting label to ClassLabel...\")\n",
    "        full_train_ds = full_train_ds.cast_column(\"label\", datasets.ClassLabel(num_classes=2, names=['Not Problematic', 'Problematic']))\n",
    "        print(f\"Features after casting: {full_train_ds.features}\")\n",
    "\n",
    "        # Split into final train and validation sets (e.g., 80/20 or 90/10)\n",
    "        split_percentage = 0.2 # Use 20% for validation\n",
    "        print(f\"Splitting into train ({1-split_percentage:.0%}) / validation ({split_percentage:.0%})...\")\n",
    "        train_val_dict = full_train_ds.train_test_split(test_size=split_percentage, stratify_by_column='label', seed=42)\n",
    "\n",
    "        final_germs_dataset_dict = datasets.DatasetDict({\n",
    "            'train': train_val_dict['train'],\n",
    "            'validation': train_val_dict['test'] # Rename 'test' split from train_test_split to 'validation'\n",
    "        })\n",
    "        print(\"Successfully created GeRMS-AT DatasetDict with Train/Validation splits:\")\n",
    "        print(final_germs_dataset_dict)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error creating/splitting DatasetDict: {e}\")\n",
    "else:\n",
    "    print(\"Skipping DatasetDict creation/splitting due to processing errors or empty DataFrame.\")\n",
    "\n",
    "\n",
    "# Save the NEW Train/Validation DatasetDict\n",
    "print(\"\\n--- Saving Processed GeRMS-AT Train/Validation Splits ---\")\n",
    "if final_germs_dataset_dict:\n",
    "    print(f\"Saving processed GeRMS-AT dataset to: {save_path_germs}\")\n",
    "    try:\n",
    "        final_germs_dataset_dict.save_to_disk(save_path_germs)\n",
    "        print(\"-> GeRMS-AT dataset saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error saving GeRMS-AT dataset: {e}\")\n",
    "else:\n",
    "    print(\"Skipping save as DatasetDict was not created.\")\n",
    "\n",
    "print(\"\\n--- GeRMS-AT Dataset Processing Complete (Train/Validation Splits Created) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tfd-wo71NCEJ"
   },
   "source": [
    "# Model-Specific Tokenization\n",
    "\n",
    "Each model requires a different tokenizer to be implemented. The code below tokenizes the pre-processes dictionaries according to the transformer model that will be used.\n",
    "\n",
    "* bert-base-multilingual-cased (mBERT)\n",
    "* xlm-roberta-base (XLM-R)\n",
    "* KB/bert-base-swedish-cased (KB-BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fcgTg5NNE73",
    "outputId": "9e0fd273-cb30-439f-aa84-1010642bb440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed datasets from disk...\n",
      "\n",
      "Successfully loaded processed DKhate dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2960\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 329\n",
      "    })\n",
      "})\n",
      "\n",
      "Successfully loaded processed BiaSWE dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n",
      "Loading processed English datasets...\n",
      "\n",
      "Successfully loaded processed English datasets:\n",
      "UD: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1828\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 458\n",
      "    })\n",
      "})\n",
      "EACL: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5264\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1303\n",
      "    })\n",
      "})\n",
      "\n",
      "UD Features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['Not Problematic', 'Problematic'], id=None)}\n",
      "EACL Features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['Not Problematic', 'Problematic'], id=None)}\n",
      "\n",
      "Attempting to concatenate datasets...\n",
      "\n",
      "Successfully concatenated English datasets:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7092\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "})\n",
      "\n",
      "Combined Train Features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['Not Problematic', 'Problematic'], id=None)}\n",
      "\n",
      "Ready to tokenize combined_eng_dataset_dict.\n",
      "\n",
      "Successfully loaded processed Germs-EVAL dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4798\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "DRIVE_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/'\n",
    "PROCESSED_DATA_PATH = DRIVE_PATH + 'processed_data/'\n",
    "\n",
    "# Define specific paths\n",
    "load_path_dk = os.path.join(PROCESSED_DATA_PATH, 'dkhate_processed_hf')\n",
    "load_path_sw = os.path.join(PROCESSED_DATA_PATH, 'biaswe_processed_hf')\n",
    "load_path_germs = os.path.join(PROCESSED_DATA_PATH, 'germs_at_processed_hf')\n",
    "load_path_ud = os.path.join(PROCESSED_DATA_PATH, 'ud_misogyny_processed_hf')\n",
    "load_path_eacl = os.path.join(PROCESSED_DATA_PATH, 'eacl_misogyny_processed_hf')\n",
    "\n",
    "# Load Datasets\n",
    "print(\"Loading processed datasets from disk...\")\n",
    "\n",
    "# Load DKhate\n",
    "try:\n",
    "    loaded_dk_dataset_dict = datasets.load_from_disk(load_path_dk)\n",
    "    print(\"\\nSuccessfully loaded processed DKhate dataset:\")\n",
    "    print(loaded_dk_dataset_dict)\n",
    "except Exception as e:\n",
    "    print(f\"\\n!! Error loading DKhate dataset from {load_path_dk}: {e}\")\n",
    "\n",
    "# Load BiaSWE\n",
    "try:\n",
    "    loaded_sw_dataset_dict = datasets.load_from_disk(load_path_sw)\n",
    "    print(\"\\nSuccessfully loaded processed BiaSWE dataset:\")\n",
    "    print(loaded_sw_dataset_dict)\n",
    "except Exception as e:\n",
    "    print(f\"\\n!! Error loading BiaSWE dataset from {load_path_sw}: {e}\")\n",
    "\n",
    "# Load & Combine English Datasets\n",
    "# Variables to hold loaded data\n",
    "loaded_ud_dataset_dict = None\n",
    "loaded_eacl_dataset_dict = None\n",
    "combined_eng_dataset_dict = None # Initialize combined dict\n",
    "\n",
    "# Load the processed datasets\n",
    "print(\"Loading processed English datasets...\")\n",
    "try:\n",
    "    loaded_ud_dataset_dict = datasets.load_from_disk(load_path_ud)\n",
    "    loaded_eacl_dataset_dict = datasets.load_from_disk(load_path_eacl)\n",
    "    print(\"\\nSuccessfully loaded processed English datasets:\")\n",
    "    print(\"UD:\", loaded_ud_dataset_dict)\n",
    "    print(\"EACL:\", loaded_eacl_dataset_dict)\n",
    "    print(\"\\nUD Features:\", loaded_ud_dataset_dict['train'].features)\n",
    "    print(\"EACL Features:\", loaded_eacl_dataset_dict['train'].features)\n",
    "\n",
    "    # Concatenate Datasets\n",
    "    print(\"\\nAttempting to concatenate datasets...\")\n",
    "    # Ensure both dictionaries and required splits exist before concatenating\n",
    "    if loaded_ud_dataset_dict and loaded_eacl_dataset_dict and \\\n",
    "       'train' in loaded_ud_dataset_dict and 'train' in loaded_eacl_dataset_dict and \\\n",
    "       'validation' in loaded_ud_dataset_dict and 'validation' in loaded_eacl_dataset_dict:\n",
    "\n",
    "        combined_eng_dataset_dict = datasets.DatasetDict({\n",
    "            'train': datasets.concatenate_datasets([loaded_ud_dataset_dict['train'], loaded_eacl_dataset_dict['train']]),\n",
    "            'validation': datasets.concatenate_datasets([loaded_ud_dataset_dict['validation'], loaded_eacl_dataset_dict['validation']])\n",
    "        })\n",
    "        print(\"\\nSuccessfully concatenated English datasets:\")\n",
    "        print(combined_eng_dataset_dict)\n",
    "        print(\"\\nCombined Train Features:\", combined_eng_dataset_dict['train'].features)\n",
    "    else:\n",
    "        print(\"!! Cannot concatenate: One or both datasets/splits are missing.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"!! Error: Could not load one or both datasets.\")\n",
    "    print(f\"  Check paths: {load_path_ud}, {load_path_eacl}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n!! An error occurred during loading or casting: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "if combined_eng_dataset_dict:\n",
    "     print(\"\\nReady to tokenize combined_eng_dataset_dict.\")\n",
    "else:\n",
    "     print(\"\\nCombined English dataset was not created due to errors.\")\n",
    "\n",
    "# Load Germs-EVAL\n",
    "try:\n",
    "    loaded_germs_dataset_dict = datasets.load_from_disk(load_path_germs)\n",
    "    print(\"\\nSuccessfully loaded processed Germs-EVAL dataset:\")\n",
    "    print(loaded_germs_dataset_dict)\n",
    "except Exception as e:\n",
    "    print(f\"\\n!! Error loading Germs-EVAL dataset from {load_path_germs}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c23243813fa7450b9ded8cc80059dafa",
      "3b18e1d8411d4775b0ee2ba5606c5078",
      "8e78899b219846d481773a4a2eeaf2f7",
      "c012d6e7ff324588a04fc5a4b2438d25",
      "cf7e3d87b88b46d3932a3f52d343b6b3",
      "6cf9b61bc3a849c0835e8e9ce16a6fc4",
      "b4c2d8dc7dfa45cbb811fd65bbe869a8",
      "e724c40323624a51b9743d1bdc2ff587",
      "b9101d0fbbe84339877cdf5bc9ad055a",
      "4c59a0bf51b94df89ee34015b2afb830",
      "e7b2e8ef5e5248a8b7ad0763720ea51d",
      "20f5fea2fabb434e82c0939baf2ab2fd",
      "36f1a28e75ce4a899e6ba09919fad789",
      "c2b9e7a49a254879829e3b827f4fb8c1",
      "2936a04d83ac415294913f3182ec9b56",
      "dbf2aaa8154248c597fd5a4634b6624b",
      "a3212b3239ca4279894b80e4b05891d8",
      "ff78c21fe09c42d4a6d4d3718a3ed31a",
      "a612192ef1d04c8babdeefc6c4f6a52a",
      "54f6692a78554cc88f709e8058ed7f13",
      "75c1cb7e743f4eb485cb9cb8c788a4fe",
      "2be4507f40ad4e249ca555b42edc567b",
      "a48a7f83e5874ad4ba972c943762577f",
      "8f0861e4eaac44e995768be1c3f03f12",
      "663790007e884c0fa09a7013e3e267cb",
      "27b43aa046384709a9509ba6caef8326",
      "7baa7aa400ce4e8eae79cffddb18801d",
      "44cb64e49d6545d9a8d282bd71fb4fca",
      "3f9bcd57ef8549ccbe00a04e477bbbe2",
      "7f6a67360fb940e88489532588ae412a",
      "81877a83e9894ae1943c76601b3ca469",
      "2b94b021f32e49338a72b4d7cd7aeeaf",
      "6648f68ab0fd487b995f215965929669",
      "016d6766e35948388f4c7c19b73e4eb4",
      "601d5064211441edba46255344286176",
      "e2ad2a935b1447ce92050372d4d2ee0d",
      "f028c34669244a32be6b0fe9e9dc8c00",
      "12211a5a05314d388174b2f09d6d90e3",
      "013040d810684158976c96253752fac2",
      "41e59d378f8c46bcb6e8370b236756f7",
      "9a9a62f7835d42a4869e0b91d5a00da8",
      "b28185f61d4b4f27adcaa27399f75b8d",
      "2cd53ac14ce84814b91eb9c73dd84fd7",
      "7237853db26f4f88b9f40f7a7a4f09b5",
      "29972009e22245a6beb71bb9f94b38df",
      "8d158682d14545a78488c2e990c03fbc",
      "97cf891708a44652a1761d77590e75a1",
      "11f30cd583ae4d83ab0af392dee8b76d",
      "6febb113778243ebac21df155364f897",
      "619d2b4ff47745e0bde86c61721a93c3",
      "9c515700916846558b86447fcfdd7653",
      "8b530abc563c47459581f15c4274fb71",
      "ac9fbafc4a904106805ed1b151aeb95c",
      "f9298988228e42cc93d42962475fdca4",
      "b4fdfa9f78784d769273cd75312321af",
      "82ccdd63ec074ba78aafcbf8fdb77c80",
      "6fab133201ec47cba7138a7305f8ba2a",
      "66ecc6aace1a4687b6e7217a790cd655",
      "6366324bb63e4541a7906cf77b479a21",
      "230158ed2a4f41e8956eed0782cea75f",
      "ff0119488a864978bd6202240965e622",
      "353d03abcf1c4113b018172d55b3d94d",
      "e0003869362247a0b6df38d13e40e13b",
      "f4b6aa323e6c4c9e8e111c13c755e48c",
      "4837a0478e22465ea9b7ec62e0fd284b",
      "990be18b6e3949bea1ff76cd31f57776",
      "1273621a6efb4caa9ef9e07b3a07a581",
      "995b9c1eecef429db12831313f359462",
      "15c1ea7ebfb949acbd41a8b9cbcfa522",
      "41f002ad97bd4a4eb8fe8089919242e7",
      "7c1115668d264634afe5cd09a0363600",
      "713aff10226b4113ae008c62e3f57f5a",
      "abbc9e9a59074a11bb71ddbebacb96e0",
      "a5a75d5b01f24ea2abe96f965bc923d7",
      "5b8dfbe84af04003bb69fb6605d1f964",
      "c5f2e747a4514b158029ae9ef3b8f53b",
      "7f28374e056f4bbf8547730abfce2e4e",
      "b3360fb934cd4efcba2a92f78cc9bdbf",
      "dbc5d95f03594a6c90f65a2b0af28eef",
      "9c991ec691c44779b805014f8e7e7e93",
      "edee95329bd54bc093d342872f78947c",
      "21b0982d51414d9f9b9059790925dc19",
      "90ab74dd94d64b91b501f467ac5edcf0",
      "477b5a31913d4d8398e4992a000c8565",
      "fb8726dee0e64745b4b3a94c24079cc6",
      "d27d9e5d226c4a2c85aac2550888046b",
      "6cfd5ce8a51b40e0a6ee52c2f26eea70",
      "265a4a4ddd404802b3a1982d0498b9a3",
      "3407c2e5fdee4d5d9103da5806b573bd",
      "3c55e23b7b6d487cabc1b7a82b49332b",
      "8b3c056b443d4208a1bd80e70060f971",
      "9940f8e9d5954448b19efbe38f6b6efd",
      "db6f5542bb814311839c86929878aed1",
      "d3f1109f5e4e46a0aecb07cfbe717f8d",
      "ac3781a099e04c28bc24ab76277ab463",
      "fdee790250434d1b8903157ac76a6de2",
      "4c7f6df4299548c59c4acd3aaf30637e",
      "9775f06af02c483aa5118d21aa333b04",
      "a87082efa7ee4f2bbcb1039e8456c0b3",
      "9f0ecb1cdf9f464dafb850da2d282a35",
      "d11e2142839c48288de2c1b8cb20a00d",
      "bec5fa53a3e848809ea5852375a50159",
      "e96f365a497441fdaa419a9c91bfa029",
      "0abe1dff7b1141de996fac1c6bac0949",
      "a0b1e8f5635440498acf46b23df1e272",
      "e37dfd2a478b4bfe90db0eecfb14b5e3",
      "03cea2010c8545a78913ade972bf0ad9",
      "32a97ccba75a4fb5a7579a70e94e972e",
      "11d811e2e9ec49409acc7fdb95767c5a",
      "0ada3edf3416414ba3cbcf9d2127e64f",
      "4ca0e3daa5d04798aa3fee6da5a479a5",
      "1ba074a1ae084acb98ff29da0c49532b",
      "9d3be6c0f00a439e87cf15c5d7f5059b",
      "02266e9c1e0c47b99cdd83bf851fc17b",
      "09b02bf209744741a2bb6938dc46386a",
      "8f3b724bf6534d96bc0c0825e459257a",
      "d9aebeedadae404993f12922c35df614",
      "921ba8279b144382aa57d497bea8a034",
      "4e27116830ba4eb1997d3f4fd0197572",
      "f9f81872e9bc4f739fd3f4d6c001ef20",
      "a64b95255db74cf8821c7eb48d7df321",
      "df276e0e991e49e29b84479c6d0f67cd",
      "d7dfddf609574b5e8b88b9fea1aeca06",
      "8842cd1221b44c14b6a2eed17de0dfa2",
      "84c9d3f1c94445aaba0d6fadfa4521e5",
      "83d4f768dfe8473ca6a9ee40c3b81714",
      "8e6d94429799475ea20b0802959ce6c6",
      "2abc6f61db094bd2b80510a805778783",
      "e96a85b48c314902aa6a372c8580615f",
      "1985fe9171514c51a6d092d60daed5e7",
      "798675acb6a44b4b943e09d415cb7c76",
      "68d37f9d63424326a077ab80b4f4b673",
      "dafb82ec731d435a8b58829a8574bb65",
      "18c4fada53984f998d5de069541c106d",
      "656c10f016424bb585428062e0a94d3c",
      "14fa03349ba649bf91a8d421f0add800",
      "519c41201d5442f094bf9f39e325a22d",
      "4deede6a4b0b4eecb5fc87d1cb312e47",
      "401db8c7e9204bf1b13d01c3c823dbc9",
      "a3c9c708256b48548e031c5c0cfdeed8",
      "8e7e7c7080ed48fc81ee0864b4fcd1e6",
      "335a4a215cbb4f29bc288f2c8b797b84",
      "7f5a9777e16d41c289536a03bd500001",
      "63123801845e4fb0b643660857c64f18",
      "82764749bec943399b3e01f9f57af6ed",
      "625773f0ff794597848ea97ff8c2d62c",
      "801f50cfaee247d7b551cff24caa673e",
      "ae8665844fa4435ca742f431b6150ae5",
      "b799fdaad5eb4a81ae0e5619972b6f1a",
      "e9e952faafb74bc59976316b4e46e65f",
      "8fa2301b5d854ff097924db1115d147b",
      "b1de0777329545519972d1c8495500c3",
      "a3511ded39764772bcade1764d65d61d",
      "9da137cdd8e44dfeb451e728aa4ec9a2",
      "5b5ed531ae7e40a3aa5aec4a68aa2eb1",
      "1350744824174b2e9a9aece34548c918",
      "9fbfd7c68ab040cf81ddf98de3f65548",
      "17c1d348ff3c41808bf6822781468104",
      "19ab52dd471f4dbbbc97550b50eda381",
      "a7e3c830d0bb45c4b2fb17b75dbf1381",
      "5410d481e1384c8f8b6528a9994a8072",
      "c6d8fe42ded544baa9a0f6aa332feaca",
      "e39c1061089149d49daeff0261295f13",
      "57b967c2358f4bd0807638473ff12259",
      "499af982ec844e32a95d415ae2b27b0a",
      "06ed729ebca54196867f0b05e52c8b51",
      "f9f7aec183244ac5b68ea0da1e9693f0",
      "d7d3a0e5c34b4f059b3964495b062183",
      "532a848b13d744d1acaa556d7f18dfc1",
      "3839d77cc72446de9de18f33bd76b6c0",
      "7ad4a3a3b4494483a702e204f7c618b6",
      "e4fa036e23124ed3883983a455f872ac",
      "644f8b77646c475a93c568d61276dff8",
      "7e575320e68345cfba453ddf3fb3a730",
      "3c2296b5e8e34b74ad900c77ef12f04f",
      "984662bb14814ff9b9ff00cb389fda16",
      "3b37c68e61f047de827d45a90918293e",
      "4235ff23adf14da3b0e75babcc1560ea",
      "06a599f11ebc4966b497759eaec47c01",
      "c13130cef60c4677aefc04bd7455476c",
      "005a9c55cf734724b210786efa76b9b3",
      "eb3a783afb2042ccbe26a3da45205adc",
      "8c919de83abe4114aab24ddedc877f22",
      "15d83e594279451ea47586686d2b6259",
      "ae4c26236d444ee8a10d9e8b4cdf02ff",
      "090bd23c78ad46c489c805f44eab9fdf",
      "51d39967ec594e78923427b1c0250440",
      "a517b6e6eeb342b5b689811ddc533340",
      "d9d527dafaac4b3199ad5e8531e53fe2",
      "8f2b708278a94b3bb42afecc611e507f",
      "a3fd2d44c6f2461aa6804371cd5e78a7",
      "c51425b3549041ce9c16b37b3d4bc29e",
      "d6918d88133f4a34be60150297bbc9e3",
      "a5a7b1f764154068b02fa107ec9f9209",
      "b5837d9d7a5343b9a8bd77d4977185fe",
      "849c4caa67b744cf89e316622a0681f8",
      "789025e7f57543a596be370399988ba6",
      "b5a729ebf12e4690b7b33435bdbba043",
      "775a43257c724daab0c8ef6d2006fb4c",
      "891737692c9e4f43839eb9d6c71417e7",
      "5c4e8aaa492e490fbdd3f5542038e818",
      "6350b6f1e811445590096de531a92486",
      "2e47e036ab5948a0a23a910a4b39e373",
      "dde69704e0554850a11f3c76e688d3ba",
      "908f6ef5a1a84831b89fee5d4bb2be8a",
      "e95d19e4d5c5409f8b339b148d255f0e",
      "6a6f54efdf6f422faeec802091befc18",
      "d9861a116bc34be2be0365019b82c286",
      "7e0159211522473099fce3f2b177b984",
      "7b812192af97443fba4723eafdb23766",
      "2a1b949df0474063a2a8f6baf91231d6",
      "3a55db8dabf44507979e0c807cf8a56c",
      "5988e8927d0d476090ebeaaba8f1afea",
      "d4c8b6f3902d4ad49681fc9497d63414",
      "ad2ecd5afc884a64abb02840d22e52af",
      "454687c8c4314037bbbe826194366018",
      "8066ab95bef94a9d91aed7a0c65d335e",
      "19c792112d474ae2ba7b035a46140665",
      "dcd69460c3a14f5e866143731bbb9718",
      "a769f5bb222b43bf8653301f0ea45bc8",
      "1d32a1cd440544b7b6e909e67ebd7579",
      "c3f2543c5ea44213ac948b14022fcff1",
      "0b128d9b2fba434da0966ba59f55ce1d",
      "a84ef2663ae84c538997614dc3266335",
      "1d7c7a348183449ba51519d5379515dc",
      "722fd47f2d0c4d79a823511407441bb8",
      "94eba6bc3d564b97a5eff948e9bb6e34",
      "c3d651b757914aafa820663577b57faf",
      "f018b527466041b2ba4269dc760b1998",
      "8cdcd062c92948b4809edf5d1d896005",
      "76531249dfae48f09872258e5a05d384",
      "99660403a9374f2791de30b914055a17",
      "a20c813b90ec4efa952f559367045e73",
      "dceecfa29973450aae8a81736693dbbd",
      "72121f50e3bb48488582dcff0aab879f",
      "b76a0b96548f40dd8a7abd2b682389f6",
      "19b97c60e63243339bfeeac093e49851",
      "9f38164257614454b99cb0466d22e012",
      "37c62791a5f24d99a48ee6c81198c58f",
      "ddf55ca51d4c41e2b0157b6ffa20bbbb",
      "aca397ab1f1443daa590f682d583d6bf",
      "b9194ad49c2f454aa4ef0ff18aed0f75",
      "6606c1f3696643ee96c633295a7d6f50",
      "601bcbbe423648d1a85e335a18c37a45",
      "579b17e39c174dd9abe4d8623ae9bb1f",
      "71e4fafb0d734112907bdea298503c2e",
      "c08ebaaf19cb48daacf94bbac478561b",
      "440b3d70bb504fcfa1a2cac9ef8c7be7",
      "cba5ab31ccc14d2093b790febe0cd101",
      "f7f047f51c9f4497b3f07df4957b1184",
      "a3d295eff44e467ba9de1a129b73de5b",
      "81683589ba22423b843e82bfc4477a8e",
      "9936a8a2260744a28be317c0a0119751",
      "20e15109dfe64ce48406cd9efd2d5ff0",
      "859552ab3dab422cbf8a418f72e1ce84",
      "7f7e0d857f8a449f9d223b6f6df63a39",
      "8292ed5877ef411986ef7c13b2e2d28b",
      "56c483fff31c4c8cb48c16a8263e1f3c",
      "63ad411eda88453e94a141d1c753a535",
      "fa7b480b31b54d6e8d29b0480eaf4e2e",
      "82553c9ddfc347f187d704caddb177e8",
      "78a42f3a5e2b48ce858d7dfa01311f90",
      "ab67d87f7723412093864d590bcfbe30",
      "4286c6c7602c4befb6718a183f6f51c5",
      "f65dc2435e0e42b4a863ef4f690876ed",
      "7dc29ecc8e7e44d29863c3ed0c587ab1",
      "342a0774c55b4d62b3d74b22f5b91e30",
      "2518ac387a044c5bb6be9484e16caade",
      "7abd74d7ff62427184e1875e4042950e",
      "636e64c986844171bbc907d605874e90",
      "303aac27bb654cf2919b5a9104d0f4ab",
      "ad544502863640aba033079daf69ba26",
      "ced52cb4c1994e4ebe8ea60bff9c6b49",
      "c964611cbc7d46f6a816ca02ee806780",
      "fc9576b7b4014b12be63ce336d87ae7f",
      "92a87b4ae6c941a8af1be5f4919cef05",
      "11c8c2fac3804511962fc95dd5268604",
      "07309e3e783a4217b29e0b7965740498",
      "1026187380584c959d74a08daa8b32ed",
      "61440eec4eb54ed5b22feee1c9eee4fa",
      "5c5025769dcb4ac49f5577c00ef188b8",
      "5c064113bd5a4c8db698c4bcb6e63194",
      "5707cf7bd2824c25960678bba9afb386",
      "4331388adb504e78830bfae662086c77",
      "e92949b6f5534b938511540e9d9ce131",
      "86e76056465d46aa982fc9d85fe03bfd",
      "baa9b790f9064d71866ae839022d4e9f",
      "6a067f78c19f4e6bbcff6fe86e6c5ba5",
      "4539861a76ec47f5abaaa7bca3b9cd0f",
      "7a359811e2074099b157206237195ba3",
      "07873d7f85454999ac7c0e0f2d834c6a",
      "34072d3738b44aa4bacd316d69fc1945",
      "8767f11c4ed345c7ab2de8800071b3b4",
      "3b677dd40d5b4364a549c7987a451a5b",
      "26383b582725463faa9936b641af91bf",
      "f1d7b22d185d402f9d224c463b8bf13b",
      "5a58d4ed2dcc4ac69224a4abe205dcaa",
      "1c4fee5691034f9d855144e643a000b8",
      "60de964a976e4de892a16b60282e301d",
      "0203d1373a464347841ee6df93a667ba",
      "1f02ae9583ab4536b7b8b75a5f7f7ffd",
      "1021c3a88273452398e39a26d2e79ddc",
      "217e65efe58347ab89af77cffa43626f",
      "2221289811614ca6a9be9bde21cad8ef",
      "b5ab83470ae0441796a4e148eb352bc9",
      "09223f01e79141408eb7d639d4d52e04",
      "a5de772b88774f6590a9a05d5f413878",
      "247e4df25e2f408d9916a5f2c006b8cd",
      "71d45bf069cd4f188fd44da2e6e43449",
      "5d8514fdd3cb446a87ab7a1be1947d6d",
      "53317af841a4456a855c4a3865fa59ca",
      "114ef61a38cf45be91ced8f15c819305",
      "5beca168e30f4d9da31e2efbf15f3ad1",
      "3d13e75edd8f40e693338b11526737a6",
      "c30d8a4d670b485d8b975c0d20d1503e",
      "5d94963deef74edaa08089423d4de1d1",
      "9637147e1f2948c5b7d95466f4729116",
      "6688e20e3ca64ba09815f3ab3d2b6cd9",
      "cdd85080d4794a778c0533eb68016506",
      "91713e7b5b5b4486b56e0cbb9f7e223f",
      "ec0fdd25d4534c8e968c7de08707c7fc",
      "83a032dd2c604fe9baafb380e91f6ec9",
      "bffebf397374446ca906b703d4c0c9ce",
      "c6177f31340b4712a36273fd5c070924",
      "fc0971ac6ed74b089e70897642ba31c6",
      "add449fa196c4909afe85fad579e12ec",
      "dff4faef8453409db5100f7f1078c68d",
      "b28639bd5911471f994ba100cd0ad422",
      "05deb8c8e8fc456a8a9b08a80f9fcff6",
      "d18b489a0f564ca4832ec209b13d0306",
      "c090a20fa71d453db77be1fc2e75809b",
      "fd8aef4482b04ab095a22955edcb2392",
      "098439a50c6d465ebf10f4387c39ddd8",
      "a8964dd1a0a1409baf5fc377917ff1fc",
      "8373bd6fd3b94fa2bfafe2f217655d43",
      "0e033462f8f3429bb75a9079f0c65ec1",
      "fc68a0fd3eeb4945b9e9417bc044e901",
      "b73af46a35b34c4d8bba02c5b14bb4f3",
      "2829851e87f845ec82be49b60ac42f41",
      "f4119f742f92484c85d93e74178821de",
      "36aed1ddaa434afbb678716a1ed54d31",
      "97fe1411ed3f4c2cb3936294610c7e0d",
      "939d8c95bddb46ef951ecc1cb6760a3a",
      "946e20f98b4049358b1323365eaaa2c7",
      "71ec54ebd67d4260b1b5025023d3236b",
      "91a714669b304b04bec42027f35cbadd",
      "f64936c66bcf49f385ae22b89a0a9a4b",
      "cf4c7951ea1149879814e19537382ff4",
      "3e8ce22a8d234e7ea5a1767a45f079b5",
      "f3d57bed7d244fd88e6fa01200f9c2c9",
      "dc79fd9f3a0c4fa2815e7c70942fba89",
      "bf988f2dd7f74f90aed90fbc5a589881",
      "a53c0a5e2a7d4ec3b8ace15172ff52f3",
      "b4eb0a37ce6a4f2599aa50e66ce4dc62",
      "db86473fbd954a62a1f387a8ead2d044",
      "583e24fba76240dd82624eff08fa55a6",
      "616a29449e1642f78c85438055751cbf",
      "9e71e99b47eb41ff8430ab953ecdaadb",
      "9ecd9371c6434260a2049be7671f1109",
      "e0a7cf6ec92a44069bfaf06c37a1a57e",
      "098e01586bd3404c944e7648b51f7065",
      "e90a4984f4384c89b84d0cb14d592c64",
      "e8ce45a8d6114fa597ffc6d90b10a632",
      "abec22c6c77a448d907b6af853bf2cf4",
      "d4f06029d1d74bf7a42d871f93350454",
      "38bdc005b12141b3a7ea3f11ac2e9ea6",
      "a777489247d142019f31d01b4c66c5da",
      "d01a96c4e9ab4d9d92a0edde62c99d98",
      "f639bfe6c88c4218ad6972055381ee3e",
      "a08a4fa1f4ae4acb84d5ee71660b47c3",
      "ce031800dc8c4fde871c220d4189973e",
      "24958218c1be47938db5b04f1514c391",
      "1fce289ddc0443109d95f8cd52a2100b",
      "8558ab9bb0c3471bab69955965f09f13",
      "e211db64107d4328a2c0b65322fa6fcb",
      "4074a2fdc31a46ff9125647723b40ff7",
      "ba9b0b2b908148ae96ae4fddb42507a6",
      "620bf02196b042ebb7be7291829c62e0",
      "5f3be66221934a51b14e54eb240ba65b",
      "7fce6452c1ba4053bd193b32e5008304",
      "d762920ecc4b4904b9271d1ae6fba9fe",
      "1e13cb81f9a3442d8a9b08f5bad744a9",
      "c83fce1194be4e5dbda1f786a036fe2f",
      "a37feb3e3d9b4df0b8686a55eb019d29",
      "6097f1d410934ad09f82702a571c7dfc",
      "77c9164694eb4eb390cb8935bd8f569e",
      "7aa71b1a7cbb4e5c8cbd344905b81387",
      "437fa206c4144c198f19d4e4cfd9bdd1",
      "496b1d7216a74697889c004dca399366",
      "b7d459d017a349a1a532664bd487cf34",
      "72737ce7ccfd411b9b3fdec044cea63c",
      "ab3971a01922474d8a36ac28af832adb",
      "9c19793d5bf846cfaa15db1543500b9c",
      "1c94d357b566461bb5d06610a6da364b",
      "2a34b56487244749b73622e03fdec83f",
      "9dac16cc889149b2974f83afa2a85345",
      "88ac8591317647a3b390122a4a5f81ad",
      "8b2735702ffc458797aa0a034132c928",
      "74337dcf246745ef9f151f3798588e72",
      "1afabfaaf6f84d9ebf1e666980210521",
      "30acb474c6994e67bc591b7951b8a1cf",
      "5a1cb0aeb8224a3181cbd09a1ddb2445",
      "d75795c5bc63410d97da251724194303",
      "f714bf7200b54880a1f1dc51d586faa2",
      "daacef5cdbc94b238f5a4a5ed01b7df7",
      "c11d0dde04b6412bbfd14c7382c0aad4",
      "fcb59cfa7ac5478db73194957fe03b32",
      "539e1c606b514d29bbd862c05bf949c1",
      "03c09f9ec2f349a1b040ec9e4112d5ee",
      "df4e457765234041ba242df8c45fb4a4",
      "eeda43880dd7426f93df1b31c5ed8c12",
      "00736d3284784ce99fdf049503b0498a",
      "4fd1dfd933d14cd886b3cb50e0557990",
      "bdddf2ccc6d2477dbdd671f5c9faf328",
      "a78c044b95fe4c4fa9ea2fc6230bfc9b",
      "788d1af9a3414b7d9cda7dc04a089a0e",
      "3c8668b0013b4b0ab9238a51480cdb13",
      "dcf1a0972b914b719168404cf0f7134f",
      "441a79ae7e23400e903b98bceac6c765",
      "4ac646d7731e4fb0a431d21461a2bdee",
      "cea1c23770e94e6aa43157084cca2ff1",
      "a09ed7da4d1d43d3aade8de7ac778554",
      "58691389967740c19c071cd413546395",
      "86b55b8f26ea4fc88e414f93a1bee714",
      "5eaafee0020d462d8d1f58f09831f713",
      "8cba8187abfa4077a3c75ced33e01c90",
      "5bfae294882e48f6ad79c4566fbde4cc",
      "2b5dbc4c16984f489c472c169e119cfd",
      "388e2a91711148f4b23aad56579a0560"
     ]
    },
    "id": "XhCYGI-31wO4",
    "outputId": "2a133c48-1bba-4575-c3a2-24f7aa9cec50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizers loaded.\n",
      "\n",
      "Tokenizing English Combined for mBERT...\n",
      "Tokenization complete for English Combined (mBERT).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/combined_english_mbert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved English Combined (mBERT) successfully.\n",
      "\n",
      "Tokenizing DKhate for mBERT...\n",
      "Tokenization complete for DKhate (mBERT).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/dkhate_mbert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved DKhate (mBERT) successfully.\n",
      "\n",
      "Tokenizing GeRMS-AT for mBERT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for GeRMS-AT (mBERT).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/germs_at_mbert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved GeRMS-AT (mBERT) successfully.\n",
      "\n",
      "Tokenizing BiaSWE for mBERT...\n",
      "Tokenization complete for BiaSWE (mBERT).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_mbert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved BiaSWE (mBERT) successfully.\n",
      "\n",
      "Tokenizing English Combined for XLM-R...\n",
      "Tokenization complete for English Combined (XLM-R).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/combined_english_xlmr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved English Combined (XLM-R) successfully.\n",
      "\n",
      "Tokenizing DKhate for XLM-R...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for DKhate (XLM-R).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/dkhate_xlmr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved DKhate (XLM-R) successfully.\n",
      "\n",
      "Tokenizing GeRMS-AT for XLM-R...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for GeRMS-AT (XLM-R).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/germs_at_xlmr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved GeRMS-AT (XLM-R) successfully.\n",
      "\n",
      "Tokenizing BiaSWE for XLM-R...\n",
      "Tokenization complete for BiaSWE (XLM-R).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_xlmr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved BiaSWE (XLM-R) successfully.\n",
      "\n",
      "Tokenizing BiaSWE for KB-BERT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for BiaSWE (KB-BERT).\n",
      "Saving tokenized data to: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_kbbert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved BiaSWE (KB-BERT) successfully.\n",
      "\n",
      "--- Tokenization Process Complete ---\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DATA_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/processed_data/'\n",
    "TOKENIZED_DATA_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/'\n",
    "os.makedirs(TOKENIZED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Define model checkpoints\n",
    "model_ckpt_mbert = \"bert-base-multilingual-cased\"\n",
    "model_ckpt_xlmr = \"xlm-roberta-base\"\n",
    "model_ckpt_kbbert = \"KB/bert-base-swedish-cased\"\n",
    "\n",
    "MAX_LENGTH = 128 # Alternative 256\n",
    "\n",
    "# Load Tokenizers\n",
    "print(\"Loading tokenizers...\")\n",
    "tokenizer_mbert = AutoTokenizer.from_pretrained(model_ckpt_mbert)\n",
    "tokenizer_xlmr = AutoTokenizer.from_pretrained(model_ckpt_xlmr)\n",
    "tokenizer_kbbert = AutoTokenizer.from_pretrained(model_ckpt_kbbert)\n",
    "print(\"Tokenizers loaded.\")\n",
    "\n",
    "# Define Tokenization Function\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"Applies tokenizer to text examples.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "# Define Function to Tokenize and Save a DatasetDict\n",
    "def tokenize_and_save(dataset_dict, tokenizer, save_path, dataset_name, model_suffix):\n",
    "    \"\"\"Tokenizes all splits in a DatasetDict and saves the result.\"\"\"\n",
    "    if dataset_dict is None:\n",
    "        print(f\"Skipping tokenization for {dataset_name} ({model_suffix}) as input is None.\")\n",
    "        return None\n",
    "    print(f\"\\nTokenizing {dataset_name} for {model_suffix}...\")\n",
    "    try:\n",
    "        tokenized_dsd = dataset_dict.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            fn_kwargs={\"tokenizer\": tokenizer}, # Pass tokenizer to the function\n",
    "            remove_columns=[\"text\"] # Remove original text column\n",
    "        )\n",
    "        # Set format for PyTorch/TF if needed later, or do it upon loading\n",
    "        # tokenized_dsd.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"]) # Adjust columns based on tokenizer type\n",
    "\n",
    "        print(f\"Tokenization complete for {dataset_name} ({model_suffix}).\")\n",
    "        print(f\"Saving tokenized data to: {save_path}\")\n",
    "        tokenized_dsd.save_to_disk(save_path)\n",
    "        print(f\"-> Saved {dataset_name} ({model_suffix}) successfully.\")\n",
    "        return tokenized_dsd\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error tokenizing/saving {dataset_name} ({model_suffix}): {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Tokenize Datasets for Each Model\n",
    "\n",
    "# For mBERT\n",
    "save_path_eng_mbert = os.path.join(TOKENIZED_DATA_PATH, 'combined_english_mbert')\n",
    "save_path_dk_mbert = os.path.join(TOKENIZED_DATA_PATH, 'dkhate_mbert')\n",
    "save_path_ge_mbert = os.path.join(TOKENIZED_DATA_PATH, 'germs_at_mbert')\n",
    "save_path_sw_mbert = os.path.join(TOKENIZED_DATA_PATH, 'biaswe_mbert')\n",
    "\n",
    "tokenized_eng_mbert = tokenize_and_save(combined_eng_dataset_dict, tokenizer_mbert, save_path_eng_mbert, \"English Combined\", \"mBERT\")\n",
    "tokenized_dk_mbert = tokenize_and_save(loaded_dk_dataset_dict, tokenizer_mbert, save_path_dk_mbert, \"DKhate\", \"mBERT\")\n",
    "tokenized_ge_mbert = tokenize_and_save(loaded_germs_dataset_dict, tokenizer_mbert, save_path_ge_mbert, \"GeRMS-AT\", \"mBERT\")\n",
    "tokenized_sw_mbert = tokenize_and_save(loaded_sw_dataset_dict, tokenizer_mbert, save_path_sw_mbert, \"BiaSWE\", \"mBERT\")\n",
    "\n",
    "# For XLM-R\n",
    "save_path_eng_xlmr = os.path.join(TOKENIZED_DATA_PATH, 'combined_english_xlmr')\n",
    "save_path_dk_xlmr = os.path.join(TOKENIZED_DATA_PATH, 'dkhate_xlmr')\n",
    "save_path_ge_xlmr = os.path.join(TOKENIZED_DATA_PATH, 'germs_at_xlmr')\n",
    "save_path_sw_xlmr = os.path.join(TOKENIZED_DATA_PATH, 'biaswe_xlmr')\n",
    "\n",
    "tokenized_eng_xlmr = tokenize_and_save(combined_eng_dataset_dict, tokenizer_xlmr, save_path_eng_xlmr, \"English Combined\", \"XLM-R\")\n",
    "tokenized_dk_xlmr = tokenize_and_save(loaded_dk_dataset_dict, tokenizer_xlmr, save_path_dk_xlmr, \"DKhate\", \"XLM-R\")\n",
    "tokenized_ge_xlmr = tokenize_and_save(loaded_germs_dataset_dict, tokenizer_xlmr, save_path_ge_xlmr, \"GeRMS-AT\", \"XLM-R\")\n",
    "tokenized_sw_xlmr = tokenize_and_save(loaded_sw_dataset_dict, tokenizer_xlmr, save_path_sw_xlmr, \"BiaSWE\", \"XLM-R\")\n",
    "\n",
    "# For KB-BERT (Swedish Data Only)\n",
    "save_path_sw_kbbert = os.path.join(TOKENIZED_DATA_PATH, 'biaswe_kbbert')\n",
    "tokenized_sw_kbbert = tokenize_and_save(loaded_sw_dataset_dict, tokenizer_kbbert, save_path_sw_kbbert, \"BiaSWE\", \"KB-BERT\")\n",
    "\n",
    "print(\"\\n--- Tokenization Process Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNElxxJdNIp5"
   },
   "source": [
    "## Training Setup using Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUkW_ixeNOHP"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch # Ensure PyTorch is available\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "# Paths where TOKENIZED data is saved\n",
    "TOKENIZED_DATA_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/'\n",
    "# Base path for saving model checkpoints and results for EACH experiment\n",
    "RESULTS_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/results/'\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Model checkpoints\n",
    "model_ckpt_mbert = \"bert-base-multilingual-cased\"\n",
    "model_ckpt_xlmr = \"xlm-roberta-base\"\n",
    "model_ckpt_kbbert = \"KB/bert-base-swedish-cased\" # Verify exact name\n",
    "\n",
    "# Define Compute Metrics Function\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Computes classification metrics.\"\"\"\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary'\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    # You can add more metrics here if needed\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define Function to Load Tokenized Data\n",
    "def load_tokenized_dataset(path: str) -> DatasetDict | None:\n",
    "    \"\"\"Loads a tokenized dataset saved to disk.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"!! Error: Dataset not found at {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        loaded_dsd = load_from_disk(path)\n",
    "        print(f\"Successfully loaded dataset from {path}\")\n",
    "        # Set format for PyTorch here!\n",
    "        # Determine columns based on model type implicitly handled by Trainer usually,\n",
    "        # but explicitly setting is safer. We'll let Trainer handle it for now,\n",
    "        # but you might need to add .set_format later if issues arise.\n",
    "        # loaded_dsd.set_format(\"torch\") # Simple version\n",
    "        print(loaded_dsd)\n",
    "        return loaded_dsd\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error loading dataset from {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Gd4A_iWwDNi"
   },
   "source": [
    "## Experiment 1: English-Only Training (XLM-R, m-BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n2lTkYnv_2l",
    "outputId": "e24265e3-aeef-4fdf-9cd6-fa14c64bf73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Experiment 1: English-Only Training ---\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/combined_english_mbert\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 7092\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/combined_english_xlmr\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 7092\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Experiment 1: English-Only Training ---\")\n",
    "\n",
    "# Load Tokenized English Data\n",
    "tokenized_eng_mbert_path = os.path.join(TOKENIZED_DATA_PATH, 'combined_english_mbert')\n",
    "tokenized_eng_xlmr_path = os.path.join(TOKENIZED_DATA_PATH, 'combined_english_xlmr')\n",
    "\n",
    "dsd_eng_mbert = load_tokenized_dataset(tokenized_eng_mbert_path)\n",
    "dsd_eng_xlmr = load_tokenized_dataset(tokenized_eng_xlmr_path)\n",
    "\n",
    "# Train Model Function\n",
    "def train_model(model_checkpoint: str,\n",
    "                tokenized_dataset: DatasetDict,\n",
    "                output_dir_suffix: str,\n",
    "                num_labels: int = 2,\n",
    "                num_epochs: int = 3,\n",
    "                learning_rate: float = 2e-5,\n",
    "                batch_size: int = 16\n",
    "                ):\n",
    "\n",
    "    run_name = f\"{os.path.basename(model_checkpoint)}_{output_dir_suffix}\"\n",
    "    output_dir = os.path.join(RESULTS_PATH, run_name)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size * 2,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        seed=42,\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(f\"Starting training for {run_name}...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"Training finished.\")\n",
    "        trainer.save_model(output_dir) # Save best model\n",
    "        print(f\"Best model saved to {output_dir}\")\n",
    "        return trainer\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error during training for {run_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovctQd5crAcX"
   },
   "source": [
    "## Run Training for Exp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8461502a365f4f419dfda1a8d7794624",
      "4093a993c5154fdca937055b4c9f81c6",
      "96b92ad4750d4347bc639ff1b4675b0d",
      "375e0fca581340a39e1482248242fd5a",
      "3f5328d095e74aa799954e8cf945ee2c",
      "3f5a32e74b564d219387976d92f9e0a8",
      "79812603393e45f6b80930e04447d710",
      "c2258798eca44bedbbb1ce2268ba7486",
      "8af83ba800f64f54810b5d8b79d9e2d7",
      "a7b6ad161dc04d95b898fb1cfd0c346e",
      "8ad2f3823f6a40f597cb6d9251189837",
      "70f823d20c164dc8a1fd581440f6a2c8",
      "70bd8ccf89bb4ba2acdb17c27afb7404",
      "aa01b05707d14d05b8ace2fe03566603",
      "6fba3bb546cd496f8ea4efdcdf9cda9c",
      "0e02175cc1814c6e86e70dbf6f0925a4",
      "8236954c7e8742bf8bd8b79cc272f42c",
      "0e96875efee44f898d77f9bd93c14b30",
      "a1780a5317cf437ab0cd7aba8d03a10f",
      "cd43ef2de11e41b58d41af9c93b78e00",
      "4d56354e93764f5792446c5878a43bda",
      "36409ffcb5304516a4921c662a1e944b",
      "f12f20ac0c2c4e709097a62f9356ae09",
      "983e90b986834fa4ae1437b818ef8566",
      "b141b7502cda4496a7db37ff10c95e21",
      "a4833160233a4b27b7a455d8459728c7",
      "cb2b3eb5ef2d4911ba2c934dc86f2d72",
      "73e6345391d741f49d50273ddce1296b",
      "eed60087245d4baf9d4243be8b723af3",
      "f64a3e177d014b7499c942ff807d501e",
      "a990bb4579a14800b632e356790f07c4",
      "eb6740e6f2264e53bb3cb83031baa305",
      "40ef97faafe84590856581d3676cbb89",
      "307c405134984b659753d2e82674cc55",
      "a9f8908246784f46a76391df247f891e",
      "c4b84d24ac7745f19e4317ebf052e1fb",
      "6016f7c359624b4887e513007a7a698f",
      "153e7baa205f4169a6c58ebc31ef2086",
      "6c8fbf6115dc4d41823f0f842dd70372",
      "7ffb4d86de7e47ecba3faa54e906dd19",
      "208140a41fc54bd68666c8f3ef2ca9ea",
      "18696ad404d2488fa0cddd490d294ff7",
      "4b08fd3b80eb42a8b70da74ea0bea5c4",
      "313145bdeeb14d629016439f78314c68"
     ]
    },
    "id": "2TkXX166Pnbx",
    "outputId": "6ba2a510-18df-43c7-c92e-a2ab8af1e1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for bert-base-multilingual-cased_mBERT_eng_only_lr2e-5_b16_ep6...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2664' max='2664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2664/2664 20:45, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.244527</td>\n",
       "      <td>0.909710</td>\n",
       "      <td>0.725389</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.283324</td>\n",
       "      <td>0.914253</td>\n",
       "      <td>0.762205</td>\n",
       "      <td>0.809365</td>\n",
       "      <td>0.720238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.319539</td>\n",
       "      <td>0.911414</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.796053</td>\n",
       "      <td>0.720238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.455421</td>\n",
       "      <td>0.897785</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.758929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.540042</td>\n",
       "      <td>0.906871</td>\n",
       "      <td>0.752266</td>\n",
       "      <td>0.763804</td>\n",
       "      <td>0.741071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.534121</td>\n",
       "      <td>0.916525</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.729167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Best model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/bert-base-multilingual-cased_mBERT_eng_only_lr2e-5_b16_ep6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for xlm-roberta-base_XLM-R_eng_only_BEST_lr2e-5_batch32_ep6...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1332' max='1332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1332/1332 20:32, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.271990</td>\n",
       "      <td>0.898921</td>\n",
       "      <td>0.689895</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.589286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.270798</td>\n",
       "      <td>0.898921</td>\n",
       "      <td>0.732733</td>\n",
       "      <td>0.739394</td>\n",
       "      <td>0.726190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>0.274108</td>\n",
       "      <td>0.903464</td>\n",
       "      <td>0.759887</td>\n",
       "      <td>0.723118</td>\n",
       "      <td>0.800595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.315028</td>\n",
       "      <td>0.911982</td>\n",
       "      <td>0.769688</td>\n",
       "      <td>0.768546</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.363766</td>\n",
       "      <td>0.913685</td>\n",
       "      <td>0.772455</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.767857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.350257</td>\n",
       "      <td>0.919364</td>\n",
       "      <td>0.780186</td>\n",
       "      <td>0.812903</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Best model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/xlm-roberta-base_XLM-R_eng_only_BEST_lr2e-5_batch32_ep6\n",
      "\n",
      "--- Experiment 1 Training Calls Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Check whether a GPU is available,\n",
    "# If so, run everything on a GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Train mBERT on English\n",
    "trainer_mbert_eng = train_model(model_ckpt_mbert, dsd_eng_mbert, \"mBERT_eng_only_lr2e-5_b16_ep6\", num_epochs=6, learning_rate=2e-5, batch_size= 16 )\n",
    "# Train XLM-R on English\n",
    "trainer_xlmr_eng = train_model(model_ckpt_xlmr, dsd_eng_xlmr, \"XLM-R_eng_only_BEST_lr2e-5_batch32_ep6\", num_epochs=6, learning_rate=2e-5, batch_size=32 )\n",
    "\n",
    "print(\"\\n--- Experiment 1 Training Calls Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llu14F1ZNZmg"
   },
   "source": [
    "## Experiment 2 (Cross-lingual Data Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWminSidCWG_",
    "outputId": "98d88ad1-5a0d-4772-bea5-2080af715243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Tokenized Datasets for Cross-Lingual Training ---\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/combined_english_mbert\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 7092\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/germs_at_mbert\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 4798\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/dkhate_mbert\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2960\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 329\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/combined_english_xlmr\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 7092\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/germs_at_xlmr\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 4798\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/dkhate_xlmr\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2960\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 329\n",
      "    })\n",
      "})\n",
      "\n",
      "Using consistent ClassLabel definition: ClassLabel(names=['Non-Problematic', 'Problematic'], id=None)\n",
      "\n",
      "--- Ensuring Consistent ClassLabel Across Datasets ---\n",
      "Casting 'train' split label to consistent ClassLabel...\n",
      "Casting 'validation' split label to consistent ClassLabel...\n",
      "Dataset features updated (example from train): {'label': ClassLabel(names=['Non-Problematic', 'Problematic'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "Casting 'train' split label to consistent ClassLabel...\n",
      "Casting 'validation' split label to consistent ClassLabel...\n",
      "Dataset features updated (example from train): {'label': ClassLabel(names=['Non-Problematic', 'Problematic'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "Casting 'train' split label to consistent ClassLabel...\n",
      "Casting 'test' split label to consistent ClassLabel...\n",
      "Dataset features updated (example from train): {'label': ClassLabel(names=['Non-Problematic', 'Problematic'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "Casting 'train' split label to consistent ClassLabel...\n",
      "Casting 'validation' split label to consistent ClassLabel...\n",
      "Dataset features updated (example from train): {'label': ClassLabel(names=['Non-Problematic', 'Problematic'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "Casting 'train' split label to consistent ClassLabel...\n",
      "Casting 'validation' split label to consistent ClassLabel...\n",
      "Dataset features updated (example from train): {'label': ClassLabel(names=['Non-Problematic', 'Problematic'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "Casting 'train' split label to consistent ClassLabel...\n",
      "Casting 'test' split label to consistent ClassLabel...\n",
      "Dataset features updated (example from train): {'label': ClassLabel(names=['Non-Problematic', 'Problematic'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "\n",
      "--- Combining Datasets for mBERT (Eng+De+Da) ---\n",
      "Combined mBERT DatasetDict (Eng+De+Da) created:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 14850\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2961\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Combining Datasets for XLM-R (Eng+De+Da) ---\n",
      "Combined XLM-R DatasetDict (Eng+De+Da) created:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 14850\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2961\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk, DatasetDict, concatenate_datasets, ClassLabel, Features, Value\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "#  Load Required Tokenized Datasets\n",
    "print(\"--- Loading Tokenized Datasets for Cross-Lingual Training ---\")\n",
    "dsd_eng_mbert = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'combined_english_mbert'))\n",
    "dsd_ge_mbert = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'germs_at_mbert'))\n",
    "dsd_dk_mbert = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'dkhate_mbert'))\n",
    "\n",
    "dsd_eng_xlmr = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'combined_english_xlmr'))\n",
    "dsd_ge_xlmr = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'germs_at_xlmr'))\n",
    "dsd_dk_xlmr = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'dkhate_xlmr'))\n",
    "\n",
    "#  Combine Datasets for mBERT\n",
    "print(\"\\n--- Combining Datasets for mBERT (Eng+De+Da) ---\")\n",
    "combined_dsd_mbert = None\n",
    "try:\n",
    "    # Check if all datasets were successfully loaded AND casted\n",
    "    if dsd_eng_mbert and dsd_ge_mbert and dsd_dk_mbert and \\\n",
    "       'train' in dsd_eng_mbert and 'train' in dsd_ge_mbert and 'train' in dsd_dk_mbert and \\\n",
    "       'validation' in dsd_eng_mbert and 'validation' in dsd_ge_mbert: # Check validation splits exist\n",
    "\n",
    "        mbert_train_splits = [dsd_eng_mbert['train'], dsd_ge_mbert['train'], dsd_dk_mbert['train']]\n",
    "        mbert_val_splits = [dsd_eng_mbert['validation'], dsd_ge_mbert['validation']]\n",
    "        # Add dkhate validation only if it exists and was processed\n",
    "        # if dsd_dk_mbert and 'validation' in dsd_dk_mbert: mbert_val_splits.append(dsd_dk_mbert['validation'])\n",
    "\n",
    "        combined_train_mbert = concatenate_datasets(mbert_train_splits)\n",
    "        combined_val_mbert = concatenate_datasets(mbert_val_splits)\n",
    "        combined_train_mbert = combined_train_mbert.shuffle(seed=42)\n",
    "\n",
    "        combined_dsd_mbert = DatasetDict({'train': combined_train_mbert,'validation': combined_val_mbert})\n",
    "        print(\"Combined mBERT DatasetDict (Eng+De+Da) created:\")\n",
    "        print(combined_dsd_mbert)\n",
    "    else:\n",
    "        print(\"!! Could not combine mBERT datasets: One or more source datasets/splits missing or failed casting.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!! Error combining datasets for mBERT: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "#  Combine Datasets for XLM-R\n",
    "print(\"\\n--- Combining Datasets for XLM-R (Eng+De+Da) ---\")\n",
    "combined_dsd_xlmr = None\n",
    "try:\n",
    "    # Check if all datasets were successfully loaded AND casted\n",
    "    if dsd_eng_xlmr and dsd_ge_xlmr and dsd_dk_xlmr and \\\n",
    "       'train' in dsd_eng_xlmr and 'train' in dsd_ge_xlmr and 'train' in dsd_dk_xlmr and \\\n",
    "       'validation' in dsd_eng_xlmr and 'validation' in dsd_ge_xlmr: # Check validation splits exist\n",
    "\n",
    "        xlmr_train_splits = [dsd_eng_xlmr['train'], dsd_ge_xlmr['train'], dsd_dk_xlmr['train']]\n",
    "        xlmr_val_splits = [dsd_eng_xlmr['validation'], dsd_ge_xlmr['validation']]\n",
    "        # Add dkhate validation only if it exists and was processed\n",
    "        # if dsd_dk_xlmr and 'validation' in dsd_dk_xlmr: xlmr_val_splits.append(dsd_dk_xlmr['validation'])\n",
    "\n",
    "        combined_train_xlmr = concatenate_datasets(xlmr_train_splits)\n",
    "        combined_val_xlmr = concatenate_datasets(xlmr_val_splits)\n",
    "        combined_train_xlmr = combined_train_xlmr.shuffle(seed=42)\n",
    "\n",
    "        combined_dsd_xlmr = DatasetDict({'train': combined_train_xlmr,'validation': combined_val_xlmr})\n",
    "        print(\"Combined XLM-R DatasetDict (Eng+De+Da) created:\")\n",
    "        print(combined_dsd_xlmr)\n",
    "    else:\n",
    "        print(\"!! Could not combine XLM-R datasets: One or more source datasets/splits missing or failed casting.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!! Error combining datasets for XLM-R: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWr9HpCkx5LI"
   },
   "source": [
    "## Cross-Lingual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940,
     "referenced_widgets": [
      "2a809d2944284cd2aa3be7de8451a4c9",
      "8598bb798e5242ed9ea0ec83e7d09350",
      "cb58e346bc1a41b7ada258fdaada2d0d",
      "a0b39690fe9243589bc16cbdc593768a",
      "d1e89081ebe6407da3ae2d221d096c34",
      "bd53f163bbf94ad9867b7d159c7ef07d",
      "297bcf31182a426289aabdc0c536de9d",
      "16874aecdbaa457883eccba71c921751",
      "d5475a92664644c8a9dfe22688f87615",
      "6956b83308bc43cf912f294769b36439",
      "e94b2058aeef4136afec7000f67a51c8",
      "188bb49a308e4c7abd7419932f580e5a",
      "34b753a9ffd644a0b924e4011c4b5921",
      "df063231b8cc4868aabeca5f31e1123b",
      "4bebd479e6c64cefb2ef46615498417d",
      "f0b063e312fd456bbb6bca3deaa8920c",
      "a070562ae6ca489f891437ef01735080",
      "d7bf18a43fb24321bc7434c592cdcff8",
      "7e21161667a64bf18247acb412cfbf48",
      "e423e8a32fc74a07b1086015e9d78ceb",
      "9e647f7406bb4e989dccb8a961f5a1a7",
      "405742164c2149e7bf66f33a2c271891"
     ]
    },
    "id": "fS9ppmTwM-Il",
    "outputId": "ff609d12-c01b-40e4-dc19-25f57fe3018c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "\n",
      "--- Training mBERT on Combined Data (Eng+De+Da) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for bert-base-multilingual-cased_eng_de_da...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5574' max='5574' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5574/5574 39:36, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.242995</td>\n",
       "      <td>0.924012</td>\n",
       "      <td>0.672489</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.576060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>0.291422</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.659443</td>\n",
       "      <td>0.869388</td>\n",
       "      <td>0.531172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.324765</td>\n",
       "      <td>0.911516</td>\n",
       "      <td>0.664962</td>\n",
       "      <td>0.682415</td>\n",
       "      <td>0.648379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.356219</td>\n",
       "      <td>0.924688</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.578554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.463005</td>\n",
       "      <td>0.913880</td>\n",
       "      <td>0.668401</td>\n",
       "      <td>0.698370</td>\n",
       "      <td>0.640898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.505368</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.660403</td>\n",
       "      <td>0.715116</td>\n",
       "      <td>0.613466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Best model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/bert-base-multilingual-cased_eng_de_da\n",
      "\n",
      "--- Training XLM-R on Combined Data (Eng+De+Da) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for xlm-roberta-base_eng_de_da...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2790' max='2790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2790/2790 38:45, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>0.319904</td>\n",
       "      <td>0.880784</td>\n",
       "      <td>0.503516</td>\n",
       "      <td>0.577419</td>\n",
       "      <td>0.446384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.288613</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.466334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.232557</td>\n",
       "      <td>0.921648</td>\n",
       "      <td>0.698701</td>\n",
       "      <td>0.728997</td>\n",
       "      <td>0.670823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.279862</td>\n",
       "      <td>0.925363</td>\n",
       "      <td>0.692629</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.620948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.317121</td>\n",
       "      <td>0.914218</td>\n",
       "      <td>0.681704</td>\n",
       "      <td>0.685139</td>\n",
       "      <td>0.678304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.337180</td>\n",
       "      <td>0.919959</td>\n",
       "      <td>0.688568</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.653367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Best model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/xlm-roberta-base_eng_de_da\n",
      "\n",
      "--- Cross-Lingual Training (Experiment 2) Calls Complete ---\n",
      "Models saved in folders ending with 'eng_de_da'\n"
     ]
    }
   ],
   "source": [
    "# Define suffix for these runs\n",
    "cross_lingual_suffix = \"eng_de_da\"\n",
    "\n",
    "# Check whether a GPU is available,\n",
    "# If so, run everything on a GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Train mBERT on Eng+De+Da\n",
    "print(\"\\n--- Training mBERT on Combined Data (Eng+De+Da) ---\")\n",
    "trainer_mbert_cross = train_model(\n",
    "    model_checkpoint=model_ckpt_mbert,\n",
    "    tokenized_dataset=combined_dsd_mbert, # Use the combined dataset\n",
    "    output_dir_suffix=cross_lingual_suffix,\n",
    "    num_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Train XLM-R on Eng+De+Da\n",
    "print(\"\\n--- Training XLM-R on Combined Data (Eng+De+Da) ---\")\n",
    "trainer_xlmr_cross = train_model(\n",
    "    model_checkpoint=model_ckpt_xlmr,\n",
    "    tokenized_dataset=combined_dsd_xlmr,\n",
    "    output_dir_suffix=cross_lingual_suffix,\n",
    "    num_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Cross-Lingual Training (Experiment 2) Calls Complete ---\")\n",
    "print(f\"Models saved in folders ending with '{cross_lingual_suffix}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KZJHCLgNdsW"
   },
   "source": [
    "## Experiment 3 (Inoculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbEO9lesNnpk",
    "outputId": "3d7c1bcf-8c07-49fb-fc50-26bb2b862f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Tokenized Swedish (BiaSWE) Data for Inoculation ---\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_mbert\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_xlmr\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n",
      "\n",
      "Creating inoculation training set (20 samples) for mBERT...\n",
      "Casting Swedish train label to ClassLabel for mBERT inoculation set...\n",
      "Inoculation set created for mBERT: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n",
      "\n",
      "Creating inoculation training set (20 samples) for XLM-R...\n",
      "Casting Swedish train label to ClassLabel for XLM-R inoculation set...\n",
      "Inoculation set created for XLM-R: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Configuration\n",
    "TOKENIZED_DATA_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/'\n",
    "RESULTS_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/results/'\n",
    "INOCULATION_RESULTS_PATH = os.path.join(RESULTS_PATH, 'inoculation_models/') # Subfolder for clarity\n",
    "os.makedirs(INOCULATION_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Model checkpoints (base names)\n",
    "model_ckpt_mbert = \"bert-base-multilingual-cased\"\n",
    "model_ckpt_xlmr = \"xlm-roberta-base\"\n",
    "\n",
    "# Paths to English only models\n",
    "best_mbert_eng_only_dir = os.path.join(RESULTS_PATH, \"bert-base-multilingual-cased_mBERT_eng_only_lr2e-5_b16_ep6\")\n",
    "best_xlmr_eng_only_dir = os.path.join(RESULTS_PATH, \"xlm-roberta-base_XLM-R_eng_only_BEST_lr2e-5_batch32_ep6\")\n",
    "\n",
    "# Load Tokenized Swedish Data\n",
    "print(\"\\n--- Loading Tokenized Swedish (BiaSWE) Data for Inoculation ---\")\n",
    "dsd_sw_mbert = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'biaswe_mbert'))\n",
    "dsd_sw_xlmr = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'biaswe_xlmr'))\n",
    "\n",
    "# Prepare Inoculation Subset\n",
    "inoculation_size = 20 # Same as KB-BERT low-data size\n",
    "dsd_sw_inoc_mbert = None\n",
    "dsd_sw_inoc_xlmr = None\n",
    "\n",
    "def create_inoculation_set(full_swedish_dsd, size, model_suffix):\n",
    "    \"\"\"Creates the small inoculation training set.\"\"\"\n",
    "    inoc_set = None\n",
    "    if full_swedish_dsd and 'train' in full_swedish_dsd:\n",
    "        train_set = full_swedish_dsd['train']\n",
    "        if len(train_set) >= size:\n",
    "            print(f\"\\nCreating inoculation training set ({size} samples) for {model_suffix}...\")\n",
    "            # Ensure ClassLabel - important if loading saved data that might not have it\n",
    "            if not isinstance(train_set.features['label'], datasets.ClassLabel):\n",
    "                 print(f\"Casting Swedish train label to ClassLabel for {model_suffix} inoculation set...\")\n",
    "                 try:\n",
    "                     cl_feature = datasets.ClassLabel(num_classes=2, names=['Not Misogyny', 'Misogyny'])\n",
    "                     train_set = train_set.cast_column(\"label\", cl_feature)\n",
    "                 except Exception as e: print(f\"Warning: Failed to cast label for {model_suffix} inoculation subset selection: {e}\")\n",
    "\n",
    "            # Attempt stratified selection\n",
    "            try:\n",
    "                 # Use test_size logic to get a small train set\n",
    "                 # train_test_split needs at least 2 samples per class for stratification usually\n",
    "                 if size >= 2 * train_set.features['label'].num_classes:\n",
    "                      inoc_split = train_set.train_test_split(train_size=size, stratify_by_column='label', seed=42)\n",
    "                      inoc_train_set = inoc_split['train']\n",
    "                 else:\n",
    "                      print(\"Size too small for stratification, selecting randomly...\")\n",
    "                      indices = np.random.choice(len(train_set), size, replace=False)\n",
    "                      inoc_train_set = train_set.select(indices)\n",
    "\n",
    "            except ValueError: # Stratification might fail\n",
    "                 print(\"Stratified selection failed. Selecting randomly...\")\n",
    "                 indices = np.random.choice(len(train_set), size, replace=False)\n",
    "                 inoc_train_set = train_set.select(indices)\n",
    "\n",
    "            inoc_set = datasets.DatasetDict({'train': inoc_train_set}) # Only need train split\n",
    "            print(f\"Inoculation set created for {model_suffix}: {inoc_set}\")\n",
    "        else:\n",
    "            print(f\"Warning: Train set size ({len(train_set)}) is less than desired inoculation_size ({size}).\")\n",
    "    else:\n",
    "        print(f\"Could not create inoculation set for {model_suffix}: Original Swedish dataset or train split not loaded.\")\n",
    "    return inoc_set\n",
    "\n",
    "dsd_sw_inoc_mbert = create_inoculation_set(dsd_sw_mbert, inoculation_size, \"mBERT\")\n",
    "dsd_sw_inoc_xlmr = create_inoculation_set(dsd_sw_xlmr, inoculation_size, \"XLM-R\")\n",
    "\n",
    "\n",
    "# Inoculation Training Function\n",
    "def run_inoculation(base_model_path: str,\n",
    "                    inoculation_dataset: DatasetDict,\n",
    "                    output_dir_suffix: str, # Specific suffix for this inoculation run\n",
    "                    num_labels: int = 2,\n",
    "                    num_epochs: int = 2,        # Fewer Epochs\n",
    "                    learning_rate: float = 1e-5 # Lower Learning Rate\n",
    "                   ):\n",
    "    \"\"\"Loads a base model, fine-tunes it on the inoculation set.\"\"\"\n",
    "    if inoculation_dataset is None or 'train' not in inoculation_dataset:\n",
    "        print(f\"Skipping inoculation for {base_model_path} - inoculation dataset invalid.\")\n",
    "        return None\n",
    "    if not os.path.exists(base_model_path):\n",
    "         print(f\"Skipping inoculation: Base model path not found at {base_model_path}\")\n",
    "         return None\n",
    "\n",
    "    print(f\"\\n--- Inoculating model from {os.path.basename(base_model_path)} ---\")\n",
    "    print(f\"    Suffix: {output_dir_suffix}, Epochs: {num_epochs}, LR: {learning_rate}\")\n",
    "\n",
    "    # Load the fine-tuned English-only model\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(base_model_path, num_labels=num_labels)\n",
    "        # Also load the corresponding tokenizer\n",
    "        # Determine original checkpoint from path if possible, otherwise use default\n",
    "        if \"bert-base-multilingual-cased\" in base_model_path:\n",
    "            tokenizer_checkpoint = \"bert-base-multilingual-cased\"\n",
    "        elif \"xlm-roberta-base\" in base_model_path:\n",
    "            tokenizer_checkpoint = \"xlm-roberta-base\"\n",
    "        else:\n",
    "            print(\"Warning: Could not determine base tokenizer from path, using mBERT default.\")\n",
    "            tokenizer_checkpoint = \"bert-base-multilingual-cased\" # Fallback\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error loading base model or tokenizer from {base_model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Define Training Arguments for inoculation\n",
    "    run_name = f\"{os.path.basename(base_model_path)}_inoc_{output_dir_suffix}\"\n",
    "    output_dir = os.path.join(INOCULATION_RESULTS_PATH, run_name)\n",
    "\n",
    "    # Calculate total steps and logging/saving steps if training for epochs on small data\n",
    "    # Avoid evaluating/saving too often on tiny data\n",
    "    num_training_steps = len(inoculation_dataset['train']) * num_epochs\n",
    "    logging_steps = max(1, num_training_steps // 5) # Log ~5 times total\n",
    "    save_steps = num_training_steps + 10 # Effectively save only at the end\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=4, #\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        # No evaluation during inoculation usually needed\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"steps\", # Save based on steps\n",
    "        save_steps=save_steps, # Save only at the end\n",
    "        logging_strategy=\"steps\", # Log based on steps\n",
    "        logging_steps=logging_steps,\n",
    "        save_total_limit=1,          # Only keep the final checkpoint\n",
    "        load_best_model_at_end=False,# Not applicable without evaluation\n",
    "        report_to=\"none\",\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Instantiate Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=inoculation_dataset[\"train\"],\n",
    "        # No eval_dataset needed\n",
    "        # compute_metrics=compute_metrics, # Not evaluating during training\n",
    "        tokenizer=tokenizer # Pass tokenizer for default collation\n",
    "    )\n",
    "\n",
    "       # Train (Inoculate)\n",
    "    print(\"Starting inoculation fine-tuning...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"Inoculation finished.\")\n",
    "        trainer.save_model(output_dir) # Save the final model state\n",
    "        tokenizer.save_pretrained(output_dir) # Save tokenizer alongside model\n",
    "        print(f\"Inoculated model saved to {output_dir}\")\n",
    "        return trainer\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error during inoculation for {run_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cXMjR6v3cvO"
   },
   "source": [
    "## Run Inoculation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711,
     "referenced_widgets": [
      "5e34461fa6dc4d15b407c5046c3f3ab9",
      "5d3dcabae54e45869348dd25a155db69",
      "e73e2020668e451daa87ede45a5f5208",
      "2c90e676fc774d3b8337741823763db5",
      "45609f9ce1a440588c17405cb1440b2e",
      "315bebd9d4d6437abb096caf7118ad86",
      "9c7c214eb0d1487cb77a892607c51285",
      "a427a72b7ca54a8db3c39dc39afe3508",
      "f40e1f1b290d4972846b73b9de9e2fa6",
      "48dd41961de34d8a95240856f079b2ed",
      "a9013bb6c7a547918812d2313a3f6bd4",
      "878157a93cdb4f08bdbdeb0c5b88f169",
      "ae3d50b1203d4b3abcf691a94acce2d8",
      "b80c250b4869421e9d302f21cac637cb",
      "2f553613a16f4d998f51fefcf2e64a24",
      "047a8d57601e4056ab17a16272f6f06e",
      "045ef8f2f18844a4b43e657f70ca2b83",
      "ecd63ee7f3b343a9bdd1ed96257f13be",
      "7cff72ea26dc46ef8b06e7c88bd727a2",
      "d43948d364d643929244e60eadc610c4",
      "337f376c3fca4079bd305d28debd63c1",
      "a8c17b90bd43418d827b34b24b93886e",
      "2def7c50dd1f4e07921d84dbdda6811c",
      "33fab69a8eab4c9492634e165b4c84d5",
      "43f458e2e2d347b3bb1e9edeeb74eb13",
      "603841b3c1c4404dac32b08e68e4abad",
      "e1a8d9ccc1f94c0a86ec5493d1c5bae8",
      "12ca51ae7f194500b09849a264c507bc",
      "2d61be3ae8c44652bc675e36af9e180e",
      "1cffef8302a94db2860d3cff8edc0893",
      "e59678dc00ac482c8e9418d5b57d9e0b",
      "216b1a33dc9945268bbafb7e629f54cf",
      "cb8e384256c740d49a44e66cb93e1be1"
     ]
    },
    "id": "XxOHapAoqa34",
    "outputId": "fb0b077c-ca73-4781-eab6-b1946d415718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "\n",
      "--- Running Inoculation for mBERT (from Eng-Only) ---\n",
      "\n",
      "--- Inoculating model from bert-base-multilingual-cased_mBERT_eng_only_lr2e-5_b16_ep6 ---\n",
      "    Suffix: inoc_20samp, Epochs: 2, LR: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-e34e1cf26590>:148: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inoculation fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.725400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inoculation finished.\n",
      "Inoculated model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/inoculation_models/bert-base-multilingual-cased_mBERT_eng_only_lr2e-5_b16_ep6_inoc_inoc_20samp\n",
      "\n",
      "--- Running Inoculation for XLM-R (from Eng-Only) ---\n",
      "\n",
      "--- Inoculating model from xlm-roberta-base_XLM-R_eng_only_BEST_lr2e-5_batch32_ep6 ---\n",
      "    Suffix: inoc_20samp, Epochs: 2, LR: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-e34e1cf26590>:148: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inoculation fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.749000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inoculation finished.\n",
      "Inoculated model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/inoculation_models/xlm-roberta-base_XLM-R_eng_only_BEST_lr2e-5_batch32_ep6_inoc_inoc_20samp\n",
      "\n",
      "--- Inoculation Training Calls Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Check whether a GPU is available,\n",
    "# If so, run everything on a GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 1. Inoculate best English-Only mBERT\n",
    "print(\"\\n--- Running Inoculation for mBERT (from Eng-Only) ---\")\n",
    "inoc_mbert_suffix = f\"inoc_{inoculation_size}samp\"\n",
    "trainer_mbert_inoc = run_inoculation(\n",
    "    base_model_path=best_mbert_eng_only_dir, # Path to best Eng-only mBERT\n",
    "    inoculation_dataset=dsd_sw_inoc_mbert,   # Swedish data tokenized for mBERT\n",
    "    output_dir_suffix=inoc_mbert_suffix,\n",
    "    num_epochs=2,\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# 2. Inoculate best English-Only XLM-R\n",
    "print(\"\\n--- Running Inoculation for XLM-R (from Eng-Only) ---\")\n",
    "inoc_xlmr_suffix = f\"inoc_{inoculation_size}samp\"\n",
    "trainer_xlmr_inoc = run_inoculation(\n",
    "    base_model_path=best_xlmr_eng_only_dir,  # Path to best Eng-only XLM-R\n",
    "    inoculation_dataset=dsd_sw_inoc_xlmr,    # Swedish data tokenized for XLM-R\n",
    "    output_dir_suffix=inoc_xlmr_suffix,\n",
    "    num_epochs=2,\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "print(\"\\n--- Inoculation Training Calls Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DQ3nSaVNn7g"
   },
   "source": [
    "## Experiment 4 (Monolingual KB-BERT vs. Multilingual Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "7jY0TqlcN5aw",
    "outputId": "ed7d3e3a-3e80-40c1-d6e0-a5f0cfaaf5b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Tokenized Swedish (BiaSWE) Data for KB-BERT ---\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_kbbert\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n",
      "\n",
      "Creating low-data training set (20 samples)...\n",
      "Casting Swedish train label to ClassLabel for low-data selection...\n",
      "Low-data Swedish dataset created:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n",
      "Using cuda device\n",
      "\n",
      "--- Training KB-BERT on High (All) Swedish Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for bert-base-swedish-cased_swe_high_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:03, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.617774</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.573200</td>\n",
       "      <td>0.551822</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.778443</td>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.521943</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>0.484443</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.501402</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>0.494953</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Best model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/bert-base-swedish-cased_swe_high_data\n",
      "\n",
      "--- KB-BERT Baseline Training Calls Complete ---\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "TOKENIZED_DATA_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/'\n",
    "RESULTS_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/results/'\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Model checkpoint for KB-BERT\n",
    "model_ckpt_kbbert = \"KB/bert-base-swedish-cased\"\n",
    "\n",
    "# Load Tokenized Swedish Data (KB-BERT version)\n",
    "print(\"\\n--- Loading Tokenized Swedish (BiaSWE) Data for KB-BERT ---\")\n",
    "tokenized_sw_kbbert_path = os.path.join(TOKENIZED_DATA_PATH, 'biaswe_kbbert')\n",
    "dsd_sw_kbbert = load_tokenized_dataset(tokenized_sw_kbbert_path)\n",
    "\n",
    "# Create Low-Data Subset\n",
    "low_data_size = 20\n",
    "dsd_sw_kbbert_low_data = None\n",
    "if dsd_sw_kbbert and 'train' in dsd_sw_kbbert and 'val' in dsd_sw_kbbert:\n",
    "    train_set_kb = dsd_sw_kbbert['train']\n",
    "    if len(train_set_kb) >= low_data_size:\n",
    "        print(f\"\\nCreating low-data training set ({low_data_size} samples)...\")\n",
    "        # Ensure ClassLabel for stratification\n",
    "        if not isinstance(train_set_kb.features['label'], datasets.ClassLabel):\n",
    "             print(\"Casting Swedish train label to ClassLabel for low-data selection...\")\n",
    "             try:\n",
    "                 class_label_feature_sw = datasets.ClassLabel(num_classes=2, names=['Not Misogyny', 'Misogyny'])\n",
    "                 # Important: Cast on a copy or reload if you need the original untyped later\n",
    "                 train_set_kb = train_set_kb.cast_column(\"label\", class_label_feature_sw)\n",
    "             except Exception as e: print(f\"Warning: Failed to cast label for subset selection: {e}\")\n",
    "\n",
    "        # Attempt stratified selection\n",
    "        try:\n",
    "             low_data_split = train_set_kb.train_test_split(train_size=low_data_size, stratify_by_column='label', seed=42)\n",
    "             low_data_train_set = low_data_split['train']\n",
    "        except ValueError: # Stratification might fail\n",
    "             print(\"Stratified selection failed (maybe too few samples per class). Selecting randomly...\")\n",
    "             indices = np.random.choice(len(train_set_kb), low_data_size, replace=False)\n",
    "             low_data_train_set = train_set_kb.select(indices)\n",
    "\n",
    "        # Create the final DatasetDict for low-data run\n",
    "        dsd_sw_kbbert_low_data = datasets.DatasetDict({\n",
    "            'train': low_data_train_set,\n",
    "            'validation': dsd_sw_kbbert['val'] # Use original full validation set\n",
    "        })\n",
    "        print(\"Low-data Swedish dataset created:\")\n",
    "        print(dsd_sw_kbbert_low_data)\n",
    "    else:\n",
    "        print(f\"Warning: Train set size ({len(train_set_kb)}) is less than desired low_data_size ({low_data_size}). Cannot create low-data set.\")\n",
    "else:\n",
    "    print(\"Could not create low-data set: Original Swedish dataset or splits not loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQnjGBBy2XrB"
   },
   "source": [
    "## Training KB-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSVfRIic2UBu"
   },
   "outputs": [],
   "source": [
    "# Check whether a GPU is available,\n",
    "# If so, run everything on a GPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 1. Train KB-BERT on Low Swedish Data\n",
    "print(\"\\n--- Training KB-BERT on Low Swedish Data ---\")\n",
    "trainer_kbbert_low = train_model(\n",
    "    model_checkpoint=model_ckpt_kbbert,\n",
    "    tokenized_dataset=dsd_sw_kbbert_low_data,\n",
    "    output_dir_suffix=\"swe_low_data\",\n",
    "    num_epochs=10,\n",
    "    learning_rate=3e-5\n",
    ")\n",
    "\n",
    "# 2. Train KB-BERT on All Swedish Data\n",
    "print(\"\\n--- Training KB-BERT on High (All) Swedish Data ---\")\n",
    "# Use standard parameters for the full dataset run\n",
    "trainer_kbbert_high = train_model(\n",
    "    model_checkpoint=model_ckpt_kbbert,\n",
    "    tokenized_dataset=dsd_sw_kbbert,\n",
    "    output_dir_suffix=\"swe_high_data\",\n",
    "    num_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "print(\"\\n--- KB-BERT Baseline Training Calls Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i65LZMQbVi3-"
   },
   "source": [
    "## Evaluation of Unseen Swedish Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4b645f7d9ede472284c3b4be466125f1",
      "a65a5ca5020f48b48bb28bc2e6119695",
      "9dda424dd7354359b49777a8cd0e06b6",
      "929b298ffedf4f8b9903f2bece034479",
      "251ec6f89d52463cb2757ab49268381c",
      "05758a24532b498b9b61626420a0aa74",
      "2a5477ccff44444ba8fd3fa1539a3e6a",
      "21302b60056c4bdcb0ea9754f6980d67",
      "ce54949e0494445ba8bbfc5a5e2d291e",
      "751d96b1d35f4e529def258fdd1754f7",
      "e483a92ee06b4f4bb9fa238fe994b839",
      "cd6db680e79747e6bb82904649082954",
      "3f063f9a1b944e71abbeb150592909bf",
      "d033de574d1e4abe8d6e34335e7a5925",
      "5eb0bf1774eb4f47824c276db01dba85",
      "d29a2236edfb4a24a80d09bbcfd0f373",
      "d3cc686b96544a929eb5a581f49a1ec5",
      "97afe25014334bddaf0ec9b34a0bd40f",
      "bd33ea2a5112407880e0ada96b2499c8",
      "1af1e1965d6741898ec4ff278b7e6f44",
      "daa1dea1cc144266b9ce188d072aa299",
      "4c5c4d8c8b5b4fd9bdd37469b96ca330",
      "955c3071411a4df5a0baa3e80c8eaf1b",
      "0456a2ef256e48ecad27b7ab6fe97cb7",
      "1dadefd390de45de94905a9f7c13a316",
      "349d09fba3814332b7c43a66a4bb0bbb",
      "023f11db655a4377b0748d35459da95c",
      "1aafdee7fbad4b429e430fcdce7c3ef0",
      "be891c3d37db423abad0e04935271772",
      "64704bd3f1fc43abbf4cf54dde7adef8",
      "48e3be02e320487d876e5746fdc711ba",
      "063f8701aacf4d2ba7fbd141c49db32d",
      "2affceefceb04da6a96f1cb212e9d6fb",
      "53b9db1178444e9785720b9ce36c6126",
      "18f188a0dc5b4774beba91e3a16aed6e",
      "d07c6de603f54797b50ff9c5284405cc",
      "ec5e1e57ad5c479ca8552e2dc73dbc33",
      "2ffad10073644b42a52b3dee8065fa72",
      "5d87d01882744002b0d87369f0156e8f",
      "72e384b1b8c448ad892732d28582bae4",
      "781727196ac14ff2871bd1799ec6b599",
      "615b61bf76ca40dfb6bf57b06059b2ec",
      "f63dee43d96a49f29d5ad0154390b80b",
      "54d79f85185145ad94526c1168791d68",
      "3f857fbab7174d2da1732dfb4ca2760a",
      "ecd821b910ab48fb83b7e5ef8935abdc",
      "c184f0e86131475a8c0367e4961c283c",
      "81f045008c974d7f9e660bb1e098b28d",
      "3c8a9a76e1d7412bbeee1a7afcf827d1",
      "370291a8f37e42fb8f706049fb71c437",
      "4a9562ba254b41bfa20f7e1f7b722297",
      "ee53eedf9a92497e939c015c6ca30bb0",
      "8506a0d1f9b647d4af266a9a5db61904",
      "0eaa49eaf57a4553853a39e87444557e",
      "2aeaf018cb3d49429165d888aabe8363"
     ]
    },
    "id": "-K5Gu7Jo8Y-E",
    "outputId": "8eeaccdc-d78c-453c-ed54-7e9e237d760c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Evaluation on Swedish Test Set ---\n",
      "\n",
      "--- Evaluating: mBERT Eng-Only (Zero-Shot) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/bert-base-multilingual-cased_mBERT_eng_only_lr2e-5_b16_ep6\n",
      "  Tokenizer: bert-base-multilingual-cased\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_mbert\n",
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 2.999316930770874, 'test_model_preparation_time': 0.0029, 'test_accuracy': 0.54, 'test_eval_f1': 0.25806451612903225, 'test_eval_precision': 0.8571428571428571, 'test_eval_recall': 0.1518987341772152, 'test_runtime': 1.2297, 'test_samples_per_second': 121.978, 'test_steps_per_second': 4.066}\n",
      "\n",
      "--- Evaluating: XLM-R Eng-Only (Zero-Shot) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/xlm-roberta-base_XLM-R_eng_only_BEST_lr2e-5_batch32_ep6\n",
      "  Tokenizer: xlm-roberta-base\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_xlmr\n",
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 2.121135950088501, 'test_model_preparation_time': 0.0031, 'test_accuracy': 0.5066666666666667, 'test_eval_f1': 0.11904761904761904, 'test_eval_precision': 1.0, 'test_eval_recall': 0.06329113924050633, 'test_runtime': 1.0834, 'test_samples_per_second': 138.451, 'test_steps_per_second': 4.615}\n",
      "\n",
      "--- Evaluating: mBERT Cross (Eng+De+Da) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/mBERT_cross_eng+de+da_lr2e-5_b16_ep6\n",
      "  Tokenizer: bert-base-multilingual-cased\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_mbert\n",
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 2.4500181674957275, 'test_model_preparation_time': 0.0051, 'test_accuracy': 0.47333333333333333, 'test_eval_f1': 0.07058823529411765, 'test_eval_precision': 0.5, 'test_eval_recall': 0.0379746835443038, 'test_runtime': 1.1381, 'test_samples_per_second': 131.796, 'test_steps_per_second': 4.393}\n",
      "\n",
      "--- Evaluating: XLM-R Cross (Eng+De+Da) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/XLM-R_cross_eng+de+da_lr2e-5_b32_ep6\n",
      "  Tokenizer: xlm-roberta-base\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_xlmr\n",
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 0.9394775629043579, 'test_model_preparation_time': 0.0073, 'test_accuracy': 0.5866666666666667, 'test_eval_f1': 0.40384615384615385, 'test_eval_precision': 0.84, 'test_eval_recall': 0.26582278481012656, 'test_runtime': 1.1642, 'test_samples_per_second': 128.848, 'test_steps_per_second': 4.295}\n",
      "\n",
      "--- Evaluating: mBERT Inoc (Eng-Only Base) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/inoculation_models/bert-base-multilingual-cased_mBERT_eng_only_lr2e-5_b16_ep6_inoc_inoc_20samp\n",
      "  Tokenizer: bert-base-multilingual-cased\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_mbert\n",
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 1.715613842010498, 'test_model_preparation_time': 0.0051, 'test_accuracy': 0.66, 'test_eval_f1': 0.5785123966942148, 'test_eval_precision': 0.8333333333333334, 'test_eval_recall': 0.4430379746835443, 'test_runtime': 1.1036, 'test_samples_per_second': 135.92, 'test_steps_per_second': 4.531}\n",
      "\n",
      "--- Evaluating: XLM-R Inoc (Eng-Only Base) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/inoculation_models/xlm-roberta-base_XLM-R_eng_only_BEST_lr2e-5_batch32_ep6_inoc_inoc_20samp\n",
      "  Tokenizer: xlm-roberta-base\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_xlmr\n",
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 0.9193404316902161, 'test_model_preparation_time': 0.0058, 'test_accuracy': 0.6866666666666666, 'test_eval_f1': 0.6356589147286822, 'test_eval_precision': 0.82, 'test_eval_recall': 0.5189873417721519, 'test_runtime': 1.1027, 'test_samples_per_second': 136.035, 'test_steps_per_second': 4.534}\n",
      "\n",
      "--- Evaluating: KB-BERT Low-Data (20 Swe) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/KB-BERT_swe_low20samp_lr3e-5_ep10\n",
      "  Tokenizer: KB/bert-base-swedish-cased\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_kbbert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 0.677925705909729, 'test_model_preparation_time': 0.0056, 'test_accuracy': 0.52, 'test_eval_f1': 0.6210526315789474, 'test_eval_precision': 0.5315315315315315, 'test_eval_recall': 0.7468354430379747, 'test_runtime': 1.1268, 'test_samples_per_second': 133.125, 'test_steps_per_second': 4.437}\n",
      "\n",
      "--- Evaluating: KB-BERT High-Data (150 Swe) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/KB-BERT_swe_high150samp_lr2e-5_b16_ep6\n",
      "  Tokenizer: KB/bert-base-swedish-cased\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_kbbert\n",
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 0.5915638208389282, 'test_model_preparation_time': 0.003, 'test_accuracy': 0.6733333333333333, 'test_eval_f1': 0.7167630057803468, 'test_eval_precision': 0.6595744680851063, 'test_eval_recall': 0.7848101265822784, 'test_runtime': 1.0938, 'test_samples_per_second': 137.136, 'test_steps_per_second': 4.571}\n",
      "\n",
      "--- Final Evaluation Results on Swedish Test Set ---\n",
      "| Experiment                  |     F1 |   Precision |   Recall |   Accuracy | Error   |\n",
      "|:----------------------------|-------:|------------:|---------:|-----------:|:--------|\n",
      "| mBERT Eng-Only (Zero-Shot)  | 0.2581 |      0.8571 |   0.1519 |     0.54   |         |\n",
      "| XLM-R Eng-Only (Zero-Shot)  | 0.119  |      1      |   0.0633 |     0.5067 |         |\n",
      "| mBERT Cross (Eng+De+Da)     | 0.0706 |      0.5    |   0.038  |     0.4733 |         |\n",
      "| XLM-R Cross (Eng+De+Da)     | 0.4038 |      0.84   |   0.2658 |     0.5867 |         |\n",
      "| mBERT Inoc (Eng-Only Base)  | 0.5785 |      0.8333 |   0.443  |     0.66   |         |\n",
      "| XLM-R Inoc (Eng-Only Base)  | 0.6357 |      0.82   |   0.519  |     0.6867 |         |\n",
      "| KB-BERT Low-Data (20 Swe)   | 0.6211 |      0.5315 |   0.7468 |     0.52   |         |\n",
      "| KB-BERT High-Data (150 Swe) | 0.7168 |      0.6596 |   0.7848 |     0.6733 |         |\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "TOKENIZED_DATA_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/'\n",
    "RESULTS_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/results/'\n",
    "INOCULATION_RESULTS_PATH = os.path.join(RESULTS_PATH, 'inoculation_models/')\n",
    "\n",
    "# Define Base Model Checkpoints\n",
    "model_ckpt_mbert = \"bert-base-multilingual-cased\"\n",
    "model_ckpt_xlmr = \"xlm-roberta-base\"\n",
    "model_ckpt_kbbert = \"KB/bert-base-swedish-cased\"\n",
    "\n",
    "# Paths to BEST Saved Model Checkpoints\n",
    "best_mbert_eng_only_dir = os.path.join(RESULTS_PATH, \"bert-base-multilingual-cased_mBERT_eng_only_lr2e-5_b16_ep6\")\n",
    "best_xlmr_eng_only_dir = os.path.join(RESULTS_PATH, \"xlm-roberta-base_XLM-R_eng_only_BEST_lr2e-5_batch32_ep6\")\n",
    "best_mbert_cross_dir = os.path.join(RESULTS_PATH, \"mBERT_cross_eng+de+da_lr2e-5_b16_ep6\")\n",
    "best_xlmr_cross_dir = os.path.join(RESULTS_PATH, \"XLM-R_cross_eng+de+da_lr2e-5_b32_ep6\")\n",
    "inoc_mbert_dir = os.path.join(INOCULATION_RESULTS_PATH, f\"{os.path.basename(best_mbert_eng_only_dir)}_inoc_inoc_20samp\")\n",
    "inoc_xlmr_dir = os.path.join(INOCULATION_RESULTS_PATH, f\"{os.path.basename(best_xlmr_eng_only_dir)}_inoc_inoc_20samp\")\n",
    "kbbert_low_dir = os.path.join(RESULTS_PATH, \"KB-BERT_swe_low20samp_lr3e-5_ep10\")\n",
    "kbbert_high_dir = os.path.join(RESULTS_PATH, \"KB-BERT_swe_high150samp_lr2e-5_b16_ep6\")\n",
    "\n",
    "# List of models/paths to evaluate\n",
    "evaluation_runs = [\n",
    "    {\"name\": \"mBERT Eng-Only (Zero-Shot)\", \"model_path\": best_mbert_eng_only_dir, \"tokenizer_name\": model_ckpt_mbert, \"test_data_suffix\": \"mbert\"},\n",
    "    {\"name\": \"XLM-R Eng-Only (Zero-Shot)\", \"model_path\": best_xlmr_eng_only_dir, \"tokenizer_name\": model_ckpt_xlmr, \"test_data_suffix\": \"xlmr\"},\n",
    "    {\"name\": \"mBERT Cross (Eng+De+Da)\",    \"model_path\": best_mbert_cross_dir, \"tokenizer_name\": model_ckpt_mbert, \"test_data_suffix\": \"mbert\"},\n",
    "    {\"name\": \"XLM-R Cross (Eng+De+Da)\",    \"model_path\": best_xlmr_cross_dir, \"tokenizer_name\": model_ckpt_xlmr, \"test_data_suffix\": \"xlmr\"},\n",
    "    {\"name\": \"mBERT Inoc (Eng-Only Base)\", \"model_path\": inoc_mbert_dir, \"tokenizer_name\": model_ckpt_mbert, \"test_data_suffix\": \"mbert\"},\n",
    "    {\"name\": \"XLM-R Inoc (Eng-Only Base)\", \"model_path\": inoc_xlmr_dir, \"tokenizer_name\": model_ckpt_xlmr, \"test_data_suffix\": \"xlmr\"},\n",
    "    {\"name\": \"KB-BERT Low-Data (20 Swe)\",  \"model_path\": kbbert_low_dir, \"tokenizer_name\": model_ckpt_kbbert, \"test_data_suffix\": \"kbbert\"},\n",
    "    {\"name\": \"KB-BERT High-Data (150 Swe)\",\"model_path\": kbbert_high_dir, \"tokenizer_name\": model_ckpt_kbbert, \"test_data_suffix\": \"kbbert\"},\n",
    "]\n",
    "\n",
    "# Load Compute Metrics Function\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'eval_f1': f1, 'eval_precision': precision, 'eval_recall': recall}\n",
    "\n",
    "\n",
    "# Evaluation Loop\n",
    "print(\"\\n--- Starting Final Evaluation on Swedish Test Set ---\")\n",
    "all_results = []\n",
    "\n",
    "# Load base Swedish tokenized data path template\n",
    "swedish_base_path = os.path.join(TOKENIZED_DATA_PATH, 'biaswe_{}') # Placeholder for suffix\n",
    "\n",
    "for run in evaluation_runs:\n",
    "    run_name = run[\"name\"]\n",
    "    model_path = run[\"model_path\"]\n",
    "    tokenizer_name = run[\"tokenizer_name\"]\n",
    "    test_data_suffix = run[\"test_data_suffix\"]\n",
    "    swedish_test_path = swedish_base_path.format(test_data_suffix)\n",
    "\n",
    "    print(f\"\\n--- Evaluating: {run_name} ---\")\n",
    "    print(f\"  Model Path: {model_path}\")\n",
    "    print(f\"  Tokenizer: {tokenizer_name}\")\n",
    "    print(f\"  Test Data: {swedish_test_path}\")\n",
    "\n",
    "    # Check if model path exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"!! Error: Model path not found. Skipping evaluation.\")\n",
    "        all_results.append({\"Experiment\": run_name, \"F1\": \"N/A\", \"Precision\": \"N/A\", \"Recall\": \"N/A\", \"Accuracy\": \"N/A\", \"Error\": \"Model Not Found\"})\n",
    "        continue\n",
    "\n",
    "    # Load Model and Tokenizer\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name) # Use original tokenizer name\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error loading model or tokenizer: {e}\")\n",
    "        all_results.append({\"Experiment\": run_name, \"F1\": \"N/A\", \"Precision\": \"N/A\", \"Recall\": \"N/A\", \"Accuracy\": \"N/A\", \"Error\": f\"Loading Failed: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Load the correct tokenized Swedish Test Set\n",
    "    try:\n",
    "        swedish_test_data = load_from_disk(swedish_test_path)['test']\n",
    "        # Optional: Set format if needed (Trainer often handles this)\n",
    "        # swedish_test_data.set_format(\"torch\")\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error loading Swedish test data from {swedish_test_path}: {e}\")\n",
    "        all_results.append({\"Experiment\": run_name, \"F1\": \"N/A\", \"Precision\": \"N/A\", \"Recall\": \"N/A\", \"Accuracy\": \"N/A\", \"Error\": f\"Test Data Load Failed: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Create dummy TrainingArguments (needed for Trainer init)\n",
    "    # Output dir is not really used here but required\n",
    "    dummy_output_dir = os.path.join(RESULTS_PATH, \"temp_evaluation_output\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=dummy_output_dir,\n",
    "        per_device_eval_batch_size=32, # Use a reasonable eval batch size\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Instantiate Trainer for prediction\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # tokenizer=tokenizer # Pass if using dynamic padding during eval (unlikely needed)\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    try:\n",
    "        print(\"Running prediction on Swedish test set...\")\n",
    "        predictions = trainer.predict(swedish_test_data)\n",
    "        # metrics dict keys will be like 'test_accuracy', 'test_eval_f1', etc.\n",
    "        metrics = predictions.metrics\n",
    "        print(\"Metrics:\", metrics)\n",
    "\n",
    "        # Extract core metrics (adjust keys based on compute_metrics output)\n",
    "        result = {\n",
    "            \"Experiment\": run_name,\n",
    "            \"F1\": metrics.get('test_eval_f1', 'N/A'), # Prefixed with 'test_' by predict\n",
    "            \"Precision\": metrics.get('test_eval_precision', 'N/A'),\n",
    "            \"Recall\": metrics.get('test_eval_recall', 'N/A'),\n",
    "            \"Accuracy\": metrics.get('test_accuracy', 'N/A'),\n",
    "            \"Error\": None\n",
    "        }\n",
    "        all_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error during prediction/evaluation for {run_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        all_results.append({\"Experiment\": run_name, \"F1\": \"N/A\", \"Precision\": \"N/A\", \"Recall\": \"N/A\", \"Accuracy\": \"N/A\", \"Error\": f\"Prediction Failed: {e}\"})\n",
    "\n",
    "# Display Results\n",
    "print(\"\\n--- Final Evaluation Results on Swedish Test Set ---\")\n",
    "results_df = pd.DataFrame(all_results)\n",
    "# Format results nicely\n",
    "results_df['F1'] = pd.to_numeric(results_df['F1'], errors='coerce').map('{:.4f}'.format)\n",
    "results_df['Precision'] = pd.to_numeric(results_df['Precision'], errors='coerce').map('{:.4f}'.format)\n",
    "results_df['Recall'] = pd.to_numeric(results_df['Recall'], errors='coerce').map('{:.4f}'.format)\n",
    "results_df['Accuracy'] = pd.to_numeric(results_df['Accuracy'], errors='coerce').map('{:.4f}'.format)\n",
    "\n",
    "print(results_df.to_markdown(index=False)) # Print markdown table\n",
    "\n",
    "# Save this DataFrame to CSV\n",
    "results_df.to_csv(os.path.join(RESULTS_PATH, \"final_evaluation_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI09ZC_gHW1O"
   },
   "source": [
    "## Experiment 5: Investigating if Scope Impacted Cross-Lingual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e74a79b5ea37451590c255c1832568f9",
      "f9088f28b11147c8aa3cce19abae2e7b",
      "2f10c580f56c459e926a5e078db32591",
      "70a5d269422c48a49b811cc9449e98d8",
      "31ba4a181b7c48cc88abc180ecf82b73",
      "36c0cb87be604d77bf12fb265e92bc87",
      "8172e6b6a54941f7a3b6b5999a61ea17",
      "0697105ce0b344c7a67c99c9fe525462",
      "1cd6a27ac98447a4964e558be04597d6",
      "398158b2432b4dcbb98f16816f3d69f3",
      "e6e797ad9af94be78f6d86e6751b6920"
     ]
    },
    "id": "PvcOqpnyAb9-",
    "outputId": "4b42d077-a9a7-47e5-caa3-cac8f3d3eed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Tokenized Datasets for Eng+De Experiment ---\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/combined_english_mbert\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 7092\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/germs_at_mbert\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 4798\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/combined_english_xlmr\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 7092\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1761\n",
      "    })\n",
      "})\n",
      "Successfully loaded dataset from /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/germs_at_xlmr\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 4798\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Combining Datasets for mBERT (Eng+De) ---\n",
      "Combined mBERT DatasetDict (Eng+De) created:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 11890\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2961\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Combining Datasets for XLM-R (Eng+De) ---\n",
      "Combined XLM-R DatasetDict (Eng+De) created:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 11890\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2961\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Training mBERT on Combined Data (Eng+De) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for bert-base-multilingual-cased_eng_de_only...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4464' max='4464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4464/4464 31:48, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.262701</td>\n",
       "      <td>0.910165</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.664234</td>\n",
       "      <td>0.680798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.303068</td>\n",
       "      <td>0.926714</td>\n",
       "      <td>0.663566</td>\n",
       "      <td>0.877049</td>\n",
       "      <td>0.533666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.303617</td>\n",
       "      <td>0.912530</td>\n",
       "      <td>0.657860</td>\n",
       "      <td>0.699438</td>\n",
       "      <td>0.620948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.395230</td>\n",
       "      <td>0.921648</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.778878</td>\n",
       "      <td>0.588529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.425911</td>\n",
       "      <td>0.921310</td>\n",
       "      <td>0.675035</td>\n",
       "      <td>0.765823</td>\n",
       "      <td>0.603491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.493345</td>\n",
       "      <td>0.913205</td>\n",
       "      <td>0.661397</td>\n",
       "      <td>0.701117</td>\n",
       "      <td>0.625935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Best model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/bert-base-multilingual-cased_eng_de_only\n",
      "\n",
      "--- Training XLM-R on Combined Data (Eng+De) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for xlm-roberta-base_eng_de_only...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2232' max='2232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2232/2232 31:12, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.310800</td>\n",
       "      <td>0.265963</td>\n",
       "      <td>0.906451</td>\n",
       "      <td>0.667467</td>\n",
       "      <td>0.643519</td>\n",
       "      <td>0.693267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215300</td>\n",
       "      <td>0.225714</td>\n",
       "      <td>0.930767</td>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.561097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.277913</td>\n",
       "      <td>0.920297</td>\n",
       "      <td>0.700508</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.688279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.296866</td>\n",
       "      <td>0.934144</td>\n",
       "      <td>0.731034</td>\n",
       "      <td>0.817901</td>\n",
       "      <td>0.660848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.351796</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.718670</td>\n",
       "      <td>0.737533</td>\n",
       "      <td>0.700748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.369574</td>\n",
       "      <td>0.924688</td>\n",
       "      <td>0.715198</td>\n",
       "      <td>0.732984</td>\n",
       "      <td>0.698254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Best model saved to /content/drive/MyDrive/SwedishHateSpeechProject/results/xlm-roberta-base_eng_de_only\n",
      "\n",
      "--- Eng+De Training (Additional Experiment) Calls Complete ---\n",
      "Models saved in folders ending with 'eng_de_only'\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk, DatasetDict, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Configuration\n",
    "TOKENIZED_DATA_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/'\n",
    "RESULTS_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/results/'\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Model checkpoints\n",
    "model_ckpt_mbert = \"bert-base-multilingual-cased\"\n",
    "model_ckpt_xlmr = \"xlm-roberta-base\"\n",
    "\n",
    "# Load Helper Functions\n",
    "# compute_metrics, load_tokenized_dataset, train_model\n",
    "\n",
    "# Load Required Tokenized Datasets for Eng+De\n",
    "print(\"--- Loading Tokenized Datasets for Eng+De Experiment ---\")\n",
    "\n",
    "# mBERT versions\n",
    "dsd_eng_mbert = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'combined_english_mbert'))\n",
    "dsd_ge_mbert = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'germs_at_mbert'))\n",
    "\n",
    "# XLM-R versions\n",
    "dsd_eng_xlmr = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'combined_english_xlmr'))\n",
    "dsd_ge_xlmr = load_tokenized_dataset(os.path.join(TOKENIZED_DATA_PATH, 'germs_at_xlmr'))\n",
    "\n",
    "\n",
    "# Combine Datasets for mBERT (Eng+De)\n",
    "print(\"\\n--- Combining Datasets for mBERT (Eng+De) ---\")\n",
    "combined_dsd_mbert_eng_de = None\n",
    "try:\n",
    "    # Ensure both necessary datasets and splits were loaded\n",
    "    if dsd_eng_mbert and dsd_ge_mbert and \\\n",
    "       'train' in dsd_eng_mbert and 'train' in dsd_ge_mbert and \\\n",
    "       'validation' in dsd_eng_mbert and 'validation' in dsd_ge_mbert:\n",
    "\n",
    "        mbert_train_splits_ed = [dsd_eng_mbert['train'], dsd_ge_mbert['train']]\n",
    "        mbert_val_splits_ed = [dsd_eng_mbert['validation'], dsd_ge_mbert['validation']]\n",
    "\n",
    "        combined_train_mbert_ed = concatenate_datasets(mbert_train_splits_ed)\n",
    "        combined_val_mbert_ed = concatenate_datasets(mbert_val_splits_ed)\n",
    "\n",
    "        # Shuffle the combined training set\n",
    "        combined_train_mbert_ed = combined_train_mbert_ed.shuffle(seed=42)\n",
    "\n",
    "        combined_dsd_mbert_eng_de = DatasetDict({\n",
    "            'train': combined_train_mbert_ed,\n",
    "            'validation': combined_val_mbert_ed\n",
    "        })\n",
    "        print(\"Combined mBERT DatasetDict (Eng+De) created:\")\n",
    "        print(combined_dsd_mbert_eng_de)\n",
    "    else:\n",
    "        print(\"!! Could not combine mBERT datasets for Eng+De: One or more source datasets/splits missing.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!! Error combining datasets for mBERT (Eng+De): {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# Combine Datasets for XLM-R (Eng+De)\n",
    "print(\"\\n--- Combining Datasets for XLM-R (Eng+De) ---\")\n",
    "combined_dsd_xlmr_eng_de = None\n",
    "try:\n",
    "     # Ensure both necessary datasets and splits were loaded\n",
    "    if dsd_eng_xlmr and dsd_ge_xlmr and \\\n",
    "       'train' in dsd_eng_xlmr and 'train' in dsd_ge_xlmr and \\\n",
    "       'validation' in dsd_eng_xlmr and 'validation' in dsd_ge_xlmr:\n",
    "\n",
    "        xlmr_train_splits_ed = [dsd_eng_xlmr['train'], dsd_ge_xlmr['train']]\n",
    "        xlmr_val_splits_ed = [dsd_eng_xlmr['validation'], dsd_ge_xlmr['validation']]\n",
    "\n",
    "        combined_train_xlmr_ed = concatenate_datasets(xlmr_train_splits_ed)\n",
    "        combined_val_xlmr_ed = concatenate_datasets(xlmr_val_splits_ed)\n",
    "\n",
    "        # Shuffle the combined training set\n",
    "        combined_train_xlmr_ed = combined_train_xlmr_ed.shuffle(seed=42)\n",
    "\n",
    "        combined_dsd_xlmr_eng_de = DatasetDict({\n",
    "            'train': combined_train_xlmr_ed,\n",
    "            'validation': combined_val_xlmr_ed\n",
    "        })\n",
    "        print(\"Combined XLM-R DatasetDict (Eng+De) created:\")\n",
    "        print(combined_dsd_xlmr_eng_de)\n",
    "    else:\n",
    "        print(\"!! Could not combine XLM-R datasets for Eng+De: One or more source datasets/splits missing.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!! Error combining datasets for XLM-R (Eng+De): {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Define suffix for these runs\n",
    "eng_de_suffix = \"eng_de_only\"\n",
    "\n",
    "# Train mBERT on Eng+De\n",
    "print(\"\\n--- Training mBERT on Combined Data (Eng+De) ---\")\n",
    "trainer_mbert_eng_de = train_model(\n",
    "    model_checkpoint=model_ckpt_mbert,\n",
    "    tokenized_dataset=combined_dsd_mbert_eng_de,\n",
    "    output_dir_suffix=eng_de_suffix,\n",
    "    num_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Train XLM-R on Eng+De\n",
    "print(\"\\n--- Training XLM-R on Combined Data (Eng+De) ---\")\n",
    "trainer_xlmr_eng_de = train_model(\n",
    "    model_checkpoint=model_ckpt_xlmr,\n",
    "    tokenized_dataset=combined_dsd_xlmr_eng_de,\n",
    "    output_dir_suffix=eng_de_suffix,\n",
    "    num_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\n--- Eng+De Training (Additional Experiment) Calls Complete ---\")\n",
    "print(f\"Models saved in folders ending with '{eng_de_suffix}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTLuEwBeT1QQ"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "fsFL9AmvT002",
    "outputId": "180c9054-cbee-4842-e292-716b6d6ebd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Evaluation on Swedish Test Set ---\n",
      "\n",
      "--- Evaluating: XLM-R Cross (Eng+De) ---\n",
      "  Model Path: /content/drive/MyDrive/SwedishHateSpeechProject/results/xlm-roberta-base_eng_de_only_lr2e-5_b32_ep6\n",
      "  Tokenizer: xlm-roberta-base\n",
      "  Test Data: /content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/biaswe_xlmr\n",
      "Running prediction on Swedish test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'test_loss': 2.5675976276397705, 'test_model_preparation_time': 0.0029, 'test_accuracy': 0.5, 'test_eval_f1': 0.0963855421686747, 'test_eval_precision': 1.0, 'test_eval_recall': 0.05063291139240506, 'test_runtime': 1.0068, 'test_samples_per_second': 148.99, 'test_steps_per_second': 4.966}\n",
      "\n",
      "--- Final Evaluation Results on Swedish Test Set ---\n",
      "| Experiment           |     F1 |   Precision |   Recall |   Accuracy | Error   |\n",
      "|:---------------------|-------:|------------:|---------:|-----------:|:--------|\n",
      "| XLM-R Cross (Eng+De) | 0.0964 |           1 |   0.0506 |        0.5 |         |\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "TOKENIZED_DATA_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/tokenized_data/'\n",
    "RESULTS_PATH = '/content/drive/MyDrive/SwedishHateSpeechProject/results/'\n",
    "INOCULATION_RESULTS_PATH = os.path.join(RESULTS_PATH, 'inoculation_models/')\n",
    "\n",
    "# Define Base Model Checkpoints\n",
    "model_ckpt_mbert = \"bert-base-multilingual-cased\"\n",
    "model_ckpt_xlmr = \"xlm-roberta-base\"\n",
    "\n",
    "# Paths to BEST Saved Model Checkpoints\n",
    "#best_mbert_eng_de_dir = os.path.join(RESULTS_PATH, \"bert-base-multilingual-cased_eng_de_only_lr2e-5_b16_ep6\")\n",
    "best_xlmr_eng_de_dir = os.path.join(RESULTS_PATH, \"xlm-roberta-base_eng_de_only_lr2e-5_b32_ep6\")\n",
    "\n",
    "# List of models/paths to evaluate\n",
    "evaluation_runs = [\n",
    "    {\"name\": \"XLM-R Cross (Eng+De)\",    \"model_path\": best_xlmr_eng_de_dir, \"tokenizer_name\": model_ckpt_xlmr, \"test_data_suffix\": \"xlmr\"},\n",
    "]\n",
    "\n",
    "# Load Compute Metrics Function\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'eval_f1': f1, 'eval_precision': precision, 'eval_recall': recall}\n",
    "\n",
    "\n",
    "# Evaluation Loop\n",
    "print(\"\\n--- Starting Final Evaluation on Swedish Test Set ---\")\n",
    "all_results = []\n",
    "\n",
    "# Load base Swedish tokenized data path template\n",
    "swedish_base_path = os.path.join(TOKENIZED_DATA_PATH, 'biaswe_{}') # Placeholder for suffix\n",
    "\n",
    "for run in evaluation_runs:\n",
    "    run_name = run[\"name\"]\n",
    "    model_path = run[\"model_path\"]\n",
    "    tokenizer_name = run[\"tokenizer_name\"]\n",
    "    test_data_suffix = run[\"test_data_suffix\"]\n",
    "    swedish_test_path = swedish_base_path.format(test_data_suffix)\n",
    "\n",
    "    print(f\"\\n--- Evaluating: {run_name} ---\")\n",
    "    print(f\"  Model Path: {model_path}\")\n",
    "    print(f\"  Tokenizer: {tokenizer_name}\")\n",
    "    print(f\"  Test Data: {swedish_test_path}\")\n",
    "\n",
    "    # Check if model path exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"!! Error: Model path not found. Skipping evaluation.\")\n",
    "        all_results.append({\"Experiment\": run_name, \"F1\": \"N/A\", \"Precision\": \"N/A\", \"Recall\": \"N/A\", \"Accuracy\": \"N/A\", \"Error\": \"Model Not Found\"})\n",
    "        continue\n",
    "\n",
    "    # Load Model and Tokenizer\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name) # Use original tokenizer name\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error loading model or tokenizer: {e}\")\n",
    "        all_results.append({\"Experiment\": run_name, \"F1\": \"N/A\", \"Precision\": \"N/A\", \"Recall\": \"N/A\", \"Accuracy\": \"N/A\", \"Error\": f\"Loading Failed: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Load the correct tokenized Swedish Test Set\n",
    "    try:\n",
    "        swedish_test_data = load_from_disk(swedish_test_path)['test']\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error loading Swedish test data from {swedish_test_path}: {e}\")\n",
    "        all_results.append({\"Experiment\": run_name, \"F1\": \"N/A\", \"Precision\": \"N/A\", \"Recall\": \"N/A\", \"Accuracy\": \"N/A\", \"Error\": f\"Test Data Load Failed: {e}\"})\n",
    "        continue\n",
    "\n",
    "    # Create dummy TrainingArguments (needed for Trainer init)\n",
    "    # Output dir is not really used here but required\n",
    "    dummy_output_dir = os.path.join(RESULTS_PATH, \"temp_evaluation_output\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=dummy_output_dir,\n",
    "        per_device_eval_batch_size=32, # Use a reasonable eval batch size\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Instantiate Trainer for prediction\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # tokenizer=tokenizer # Pass if using dynamic padding during eval (unlikely needed)\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    try:\n",
    "        print(\"Running prediction on Swedish test set...\")\n",
    "        predictions = trainer.predict(swedish_test_data)\n",
    "        # metrics dict keys will be like 'test_accuracy', 'test_eval_f1', etc.\n",
    "        metrics = predictions.metrics\n",
    "        print(\"Metrics:\", metrics)\n",
    "\n",
    "        # Extract core metrics (adjust keys based on compute_metrics output)\n",
    "        result = {\n",
    "            \"Experiment\": run_name,\n",
    "            \"F1\": metrics.get('test_eval_f1', 'N/A'), # Prefixed with 'test_' by predict\n",
    "            \"Precision\": metrics.get('test_eval_precision', 'N/A'),\n",
    "            \"Recall\": metrics.get('test_eval_recall', 'N/A'),\n",
    "            \"Accuracy\": metrics.get('test_accuracy', 'N/A'),\n",
    "            \"Error\": None\n",
    "        }\n",
    "        all_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!! Error during prediction/evaluation for {run_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        all_results.append({\"Experiment\": run_name, \"F1\": \"N/A\", \"Precision\": \"N/A\", \"Recall\": \"N/A\", \"Accuracy\": \"N/A\", \"Error\": f\"Prediction Failed: {e}\"})\n",
    "\n",
    "# Display Results\n",
    "print(\"\\n--- Final Evaluation Results on Swedish Test Set ---\")\n",
    "results_df = pd.DataFrame(all_results)\n",
    "# Format results nicely\n",
    "results_df['F1'] = pd.to_numeric(results_df['F1'], errors='coerce').map('{:.4f}'.format)\n",
    "results_df['Precision'] = pd.to_numeric(results_df['Precision'], errors='coerce').map('{:.4f}'.format)\n",
    "results_df['Recall'] = pd.to_numeric(results_df['Recall'], errors='coerce').map('{:.4f}'.format)\n",
    "results_df['Accuracy'] = pd.to_numeric(results_df['Accuracy'], errors='coerce').map('{:.4f}'.format)\n",
    "\n",
    "print(results_df.to_markdown(index=False)) # Print markdown table\n",
    "\n",
    "# Save this DataFrame to CSV\n",
    "results_df.to_csv(os.path.join(RESULTS_PATH, \"final_evaluation_results.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
